{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4676fd0a-9856-4ef7-b01a-ca80a9e5a22c",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2acf1eff-699f-451f-88ec-4dcb7d5d7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from pyverilog.vparser.parser import parse\n",
    "from pyverilog.vparser.ast import (\n",
    "    # Node types we'll explicitly handle\n",
    "    Source,\n",
    "    Description,\n",
    "    ModuleDef,\n",
    "    Always,\n",
    "    SensList,\n",
    "    IfStatement,\n",
    "    Assign,\n",
    "    BlockingSubstitution,\n",
    "    NonblockingSubstitution,\n",
    "    Eq,\n",
    "    Or,\n",
    "    GreaterThan,\n",
    "    Identifier,\n",
    "    IntConst,\n",
    "    Cond,\n",
    "    Block,\n",
    "    Lvalue,\n",
    "    Rvalue\n",
    ")\n",
    "from pyverilog.vparser.ast import *  # Import Always block\n",
    "from pyverilog.ast_code_generator.codegen import ASTCodeGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8ac3cf0f-9a1b-461c-83ce-84feb51210ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains.qa_generation.prompt import CHAT_PROMPT\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0053be9-1c11-4506-b68d-16fababcd306",
   "metadata": {},
   "source": [
    "### OPEN AI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "140ec5ec-f2db-4096-a8f3-8bcb938b1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up the OpenAI LLM\n",
    "llm_OAI = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\", # \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "    # model=\"gpt-4o-mini-2024-07-18\",\n",
    "    # model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=\"\",  # Replace with your OpenAI API key\n",
    "    # temperature=1,  # Adjust the creativity level\n",
    "    # max_tokens=1000,   # Set the maximum output token limit\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "29dcd685-8bb5-4f0e-b57b-4b4cbd743be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key = \"\",\n",
    "    organization= None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569655a-89ab-410a-a811-5d59a45ad7a1",
   "metadata": {},
   "source": [
    "### Open source LLMA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "61f4f36d-1ba5-4b1e-b000-3ca18b880e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGroqLLM(LLM):\n",
    "    def __init__(self, groq_api_key, model_name):\n",
    "        client = Groq(\n",
    "            api_key= groq_api_key,  # Replace with your Groq API key\n",
    "        )\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        \"\"\"\n",
    "        Call the underlying ChatGroq LLM with the given prompt and return the response.\n",
    "        \"\"\"\n",
    "        chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model= model_name,\n",
    "        )\n",
    "        # Use the generate method with prompt directly\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        \n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat_groq\"\n",
    "groq_api_key = \"\"\n",
    "# groq_api_key = \"\"\n",
    "# Initialize Groq Langchain chat object and conversation\n",
    "groq_chat = ChatGroq(\n",
    "        groq_api_key=groq_api_key, \n",
    "        model_name=\"llama-3.3-70b-versatile\"\n",
    "        # model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b488c6-448a-4c22-af5c-b23a901bf768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### AST Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "24f6f0c2-576f-45fe-b12b-ff579daca167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeVisitor:\n",
    "    \"\"\"\n",
    "    A simple, robust visitor that attempts to handle cases where\n",
    "    node.children() returns either:\n",
    "      - A list of (attribute_name, child_node) pairs,\n",
    "      - A list of child_nodes, or\n",
    "      - A single object (like 'Description') instead of a list.\n",
    "    \"\"\"\n",
    "\n",
    "    def visit(self, node):\n",
    "        \"\"\"Dispatch to a method named visit_<NodeClassName>.\"\"\"\n",
    "        method_name = \"visit_\" + node.__class__.__name__\n",
    "        visitor = getattr(self, method_name, self.generic_visit)\n",
    "        return visitor(node)\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        \"\"\"Fallback for node types without a custom visit_* method.\"\"\"\n",
    "        # Check if node has a .children() method\n",
    "        if hasattr(node, \"children\") and callable(node.children):\n",
    "            children = node.children()\n",
    "            \n",
    "            # If .children() returned a single item (not a list/tuple):\n",
    "            if not isinstance(children, (list, tuple)):\n",
    "                # Just treat it as one \"child\"\n",
    "                children = [children]\n",
    "\n",
    "            for child in children:\n",
    "                # Sometimes child is (attr_name, child_node), sometimes just child_node\n",
    "                if isinstance(child, tuple) and len(child) == 2:\n",
    "                    attr_name, child_node = child\n",
    "                    self.visit(child_node)\n",
    "                else:\n",
    "                    # child itself is presumably a node\n",
    "                    self.visit(child)\n",
    "        # If no children, do nothing\n",
    "        return\n",
    "###############################################################################\n",
    "# Extractor\n",
    "###############################################################################\n",
    "class VerilogInfoExtractor(NodeVisitor):\n",
    "    \"\"\"\n",
    "    Walks the AST, collecting:\n",
    "      - always blocks\n",
    "      - continuous assign statements\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.always_blocks = []\n",
    "        self.assign_statements = []\n",
    "        self.module_instantiations = []  # NEW: For module calls\n",
    "\n",
    "    # ------------------------------------\n",
    "    # MODULE INSTANTIATIONS\n",
    "    # ------------------------------------\n",
    "    def visit_InstanceList(self, node):\n",
    "        \"\"\"\n",
    "        Handle a list of module instances (e.g., u_submodule, u_another_module).\n",
    "        \"\"\"\n",
    "        for instance in node.instances:\n",
    "            self.visit_Instance(instance)\n",
    "\n",
    "    def visit_Instance(self, node):\n",
    "        \"\"\"\n",
    "        Handle a single module instance.\n",
    "        \"\"\"\n",
    "        instance_info = {\n",
    "            \"module_name\": node.module,  # Module being instantiated\n",
    "            \"instance_name\": node.name,  # Name of the instance (e.g., u_submodule)\n",
    "            \"port_connections\": []\n",
    "        }\n",
    "\n",
    "        # Extract port connections\n",
    "        for connection in node.portlist:\n",
    "            port_name = connection.portname  # Name of the module's port\n",
    "            signal_name = self.stringify_condition(connection.argname)  # Signal connected to this port\n",
    "            instance_info[\"port_connections\"].append({\n",
    "                \"port\": port_name,\n",
    "                \"signal\": signal_name\n",
    "            })\n",
    "\n",
    "        # Append to the list of module instantiations\n",
    "        self.module_instantiations.append(instance_info)\n",
    "\n",
    "        # Continue visiting deeper nodes if needed\n",
    "        self.generic_visit(node)        \n",
    "\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Existing logic for Always, IfStatement, etc. omitted\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    def extract_case_statement(self, casenode):\n",
    "        \"\"\"\n",
    "        Convert a CaseStatement (or Casex/Casez) into a structured dict.\n",
    "        Example output:\n",
    "        {\n",
    "          \"type\": \"case_statement\",\n",
    "          \"case_expr\": \"opcode\",   # e.g. the expression after 'case(...)'\n",
    "          \"cases\": [\n",
    "            {\n",
    "              \"conditions\": [\"4'd0\"],     # e.g. case item expression\n",
    "              \"statements\": [ ... ]       # the statements for that case\n",
    "            },\n",
    "            {\n",
    "              \"conditions\": [\"4'd1\", \"4'd2\"], \n",
    "              \"statements\": [ ... ]\n",
    "            },\n",
    "            {\n",
    "              \"conditions\": [\"default\"],\n",
    "              \"statements\": [ ... ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Stringify the main case expression\n",
    "        case_expr_str = self.stringify_condition(casenode.comp)\n",
    "\n",
    "        case_items_info = []\n",
    "        for case_item in casenode.caselist:\n",
    "            # Each CaseItem has a list of conditions, or empty for default\n",
    "            if not case_item.cond:\n",
    "                # default\n",
    "                cond_strs = [\"default\"]\n",
    "            else:\n",
    "                cond_strs = [self.stringify_condition(cond) for cond in case_item.cond]\n",
    "\n",
    "            # The statement might be a single statement or a block\n",
    "            statements_info = self.extract_case_item_statements(case_item.statement)\n",
    "\n",
    "            case_items_info.append({\n",
    "                \"conditions\": cond_strs,\n",
    "                \"statements\": statements_info,\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"type\": \"case_statement\",\n",
    "            \"case_expr\": case_expr_str,\n",
    "            \"cases\": case_items_info,\n",
    "        }\n",
    "\n",
    "    def extract_case_item_statements(self, stmt):\n",
    "        \"\"\"\n",
    "        Extract the statements that appear under a single CaseItem.\n",
    "        Typically, you'd reuse your existing helpers (like extract_assignments, etc.)\n",
    "        \"\"\"\n",
    "\n",
    "        statements_data = []\n",
    "\n",
    "        if stmt is None:\n",
    "            return statements_data\n",
    "\n",
    "        if isinstance(stmt, Block):\n",
    "            # If the case item is a block, handle each statement in it\n",
    "            for s in stmt.statements:\n",
    "                statements_data.extend(self.extract_case_item_statements(s))\n",
    "        elif isinstance(stmt, IfStatement):\n",
    "            # Potentially handle nested if\n",
    "            statements_data.append({\n",
    "                \"type\": \"if_statement\",\n",
    "                \"info\": self.extract_if_statement(stmt)\n",
    "            })\n",
    "        elif isinstance(stmt, (BlockingSubstitution, NonblockingSubstitution)):\n",
    "            lhs_str = self.stringify_condition(stmt.left.var)\n",
    "            rhs_str = self.stringify_condition(stmt.right.var)\n",
    "            statements_data.append({\n",
    "                \"type\": \"assignment\",\n",
    "                \"lhs\": lhs_str,\n",
    "                \"rhs\": rhs_str\n",
    "            })\n",
    "        elif isinstance(stmt, Assign):\n",
    "            lhs_str = self.stringify_condition(stmt.left)\n",
    "            rhs_str = self.stringify_condition(stmt.right)\n",
    "            statements_data.append({\n",
    "                \"type\": \"continuous_assign\",\n",
    "                \"lhs\": lhs_str,\n",
    "                \"rhs\": rhs_str\n",
    "            })\n",
    "        elif isinstance(stmt, CaseStatement):\n",
    "            # Nested case statement\n",
    "            statements_data.append(self.extract_case_statement(stmt))\n",
    "        else:\n",
    "            # Fallback for any unhandled statement type\n",
    "            statements_data.append({\n",
    "                \"type\": \"unhandled\",\n",
    "                \"node_class\": stmt.__class__.__name__\n",
    "            })\n",
    "\n",
    "        return statements_data\n",
    "\n",
    "    # ------------------------------------\n",
    "    # ALWAYS BLOCKS\n",
    "    # ------------------------------------\n",
    "    def visit_Always(self, node):\n",
    "        \"\"\"\n",
    "        Collect data from an always block:\n",
    "        1) The sensitivity list (triggers)\n",
    "        2) The sequential logic (if-else, assignments, etc.)\n",
    "        3) Post-logic assignments\n",
    "        \"\"\"\n",
    "        block_info = {\n",
    "            \"block_type\": \"always\",\n",
    "            \"triggers\": self.extract_triggers(node.sens_list),\n",
    "            \"logic\": [],\n",
    "            \"post_logic_assignments\": []\n",
    "        }\n",
    "\n",
    "        # The statement(s) inside the always block\n",
    "        statements = node.statement\n",
    "        logic_blocks, tail_assigns = self.extract_logic(statements)\n",
    "\n",
    "        block_info[\"logic\"] = logic_blocks\n",
    "        block_info[\"post_logic_assignments\"] = tail_assigns\n",
    "\n",
    "        self.always_blocks.append(block_info)\n",
    "\n",
    "        # Also visit children in case of nested always or deeper nodes (rare but possible)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Helper Methods\n",
    "    ###########################################################################\n",
    "    def extract_triggers(self, sens_list):\n",
    "        \"\"\"\n",
    "        Parse the sensitivity list to get triggers like 'posedge clk' or 'negedge reset'.\n",
    "        \"\"\"\n",
    "        triggers = []\n",
    "        if isinstance(sens_list, SensList):\n",
    "            for sens_item in sens_list.list:\n",
    "                edge_type = getattr(sens_item.sig, 'clockedge', None)  # 'posedge', 'negedge', or None\n",
    "                sig_name = getattr(sens_item.sig, 'name', None)\n",
    "                if edge_type and sig_name:\n",
    "                    triggers.append(f\"{edge_type} {sig_name}\")\n",
    "                elif sig_name:\n",
    "                    triggers.append(sig_name)\n",
    "        return triggers\n",
    "\n",
    "    def extract_logic(self, statement):\n",
    "        \"\"\"\n",
    "        Traverse the statement(s) inside the always block to collect:\n",
    "         - if-else (nested) chains\n",
    "         - assignments after or outside if-else\n",
    "        \"\"\"\n",
    "        logic_blocks = []\n",
    "        tail_assignments = []\n",
    "\n",
    "        if isinstance(statement, Block):\n",
    "            # Iterate over each statement in the block\n",
    "            for stmt in statement.statements:\n",
    "                if isinstance(stmt, IfStatement):\n",
    "                    logic_blocks.extend(self.extract_if_statement(stmt))\n",
    "                elif isinstance(stmt, CaseStatement):\n",
    "                    case_info = self.extract_case_statement(stmt)\n",
    "                    logic_blocks.append(case_info)\n",
    "                elif isinstance(stmt, ForStatement):\n",
    "                    # Handle for-loop\n",
    "                    loop_info = self.visit_ForStatement(stmt)\n",
    "                    logic_blocks.append(loop_info)\n",
    "                else:\n",
    "                    tail_assignments.extend(self.extract_assignments(stmt))\n",
    "\n",
    "        elif isinstance(statement, IfStatement):\n",
    "            # Single IfStatement at top-level\n",
    "            logic_blocks.extend(self.extract_if_statement(statement))\n",
    "        elif isinstance(statement, CaseStatement):\n",
    "            case_info = self.extract_case_statement(statement)\n",
    "            logic_blocks.append(case_info)\n",
    "        elif isinstance(statement, ForStatement):\n",
    "            # Single for-loop at top-level\n",
    "            loop_info = self.visit_ForStatement(statement)\n",
    "            logic_blocks.append(loop_info)\n",
    "\n",
    "        else:\n",
    "            # Possibly a single assignment at top-level\n",
    "            tail_assignments.extend(self.extract_assignments(statement))\n",
    "\n",
    "        return logic_blocks, tail_assignments\n",
    "\n",
    "    def extract_if_statement(self, ifstmt):\n",
    "        \"\"\"\n",
    "        Return a list of if/else blocks in structured form, e.g.:\n",
    "        [\n",
    "          {\n",
    "            \"condition\": \"<expression>\",\n",
    "            \"assignments\": [{ \"lhs\": \"data\", \"rhs\": \"8'b0\" }, ... ]\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "\n",
    "        # If part\n",
    "        cond_str = self.stringify_condition(ifstmt.cond)\n",
    "        if_assigns = self.extract_assignments(ifstmt.true_statement)\n",
    "        blocks.append({\n",
    "            \"condition\": cond_str,\n",
    "            \"assignments\": if_assigns\n",
    "        })\n",
    "\n",
    "        # Else or else-if\n",
    "        false_part = ifstmt.false_statement\n",
    "        if false_part is not None:\n",
    "            if isinstance(false_part, IfStatement):\n",
    "                # Recursively extract nested else-if\n",
    "                blocks.extend(self.extract_if_statement(false_part))\n",
    "            else:\n",
    "                # It's a direct else block or single statement\n",
    "                else_assigns = self.extract_assignments(false_part)\n",
    "                blocks.append({\n",
    "                    \"condition\": \"else\",\n",
    "                    \"assignments\": else_assigns\n",
    "                })\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def extract_assignments(self, stmt):\n",
    "        \"\"\"\n",
    "        Return a list of assignments in the form: [ { \"lhs\": \"...\", \"rhs\": \"...\" }, ... ]\n",
    "        \"\"\"\n",
    "        assignments = []\n",
    "        if stmt is None:\n",
    "            return assignments\n",
    "            \n",
    "        if isinstance(stmt, Block):\n",
    "            for s in stmt.statements:\n",
    "                assignments.extend(self.extract_assignments(s))\n",
    "\n",
    "        elif isinstance(stmt, (NonblockingSubstitution, BlockingSubstitution)):\n",
    "            lhs_str = self.stringify_condition(stmt.left.var)\n",
    "            rhs_str = self.stringify_condition(stmt.right.var)\n",
    "            assignments.append({\"lhs\": lhs_str, \"rhs\": rhs_str})\n",
    "\n",
    "        elif isinstance(stmt, Assign):\n",
    "            # Continuous assignment - rarely inside an always, but included for completeness\n",
    "            lhs_str = self.stringify_condition(stmt.left)\n",
    "            rhs_str = self.stringify_condition(stmt.right)\n",
    "            assignments.append({\"lhs\": lhs_str, \"rhs\": rhs_str})\n",
    "\n",
    "        elif isinstance(stmt, IfStatement):\n",
    "            # If statement directly in a block: treat it separately if needed\n",
    "            pass\n",
    "\n",
    "        return assignments\n",
    "    # ------------------------------------\n",
    "    # CONTINUOUS ASSIGN STATEMENTS\n",
    "    # ------------------------------------\n",
    "    def visit_Assign(self, node):\n",
    "        \"\"\"\n",
    "        Capture 'assign lhs = rhs;' statements\n",
    "        \"\"\"\n",
    "        lhs_str = self.stringify_condition(node.left.var)\n",
    "        rhs_str = self.stringify_condition(node.right.var)\n",
    "        self.assign_statements.append({\n",
    "            \"lhs\": lhs_str,\n",
    "            \"rhs\": rhs_str\n",
    "        })\n",
    "\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def stringify_condition(self, cond):\n",
    "        \"\"\"\n",
    "        Convert AST condition nodes into readable strings.\n",
    "        \"\"\"\n",
    "        if cond is None:\n",
    "            return \"None\"\n",
    "        # Logical operators\n",
    "        if isinstance(cond, And):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} AND {right})\"\n",
    "        if isinstance(cond, Land):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} ANDAND {right})\"\n",
    "        elif isinstance(cond, Or):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} OR {right})\"\n",
    "        elif isinstance(cond, Lor):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} OROR {right})\"\n",
    "        # Arithmetic operations\n",
    "        elif isinstance(cond, Plus):  # Handle addition\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} PLUS {right})\"\n",
    "        elif isinstance(cond, Minus):  # Handle subtraction\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} MINUS {right})\"\n",
    "\n",
    "        elif isinstance(cond, Ulnot):\n",
    "            operand = cond.children()[0]\n",
    "            operand_str = self.stringify_condition(operand)\n",
    "            return f\"NOT ({operand_str})\"\n",
    "        elif isinstance(cond, Unot):\n",
    "            operand = cond.children()[0]\n",
    "            operand_str = self.stringify_condition(operand)\n",
    "            return f\"NOT ({operand_str})\"\n",
    "        elif isinstance(cond, Uand):\n",
    "            operand = cond.children()[0]\n",
    "            operand_str = self.stringify_condition(operand)\n",
    "            return f\"AND ({operand_str})\"\n",
    "        elif isinstance(cond, GreaterThan):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"{left} GREATER THAN {right}\"\n",
    "        elif isinstance(cond, LessThan):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"{left} LESS THAN {right}\"\n",
    "        elif isinstance(cond, Eq):\n",
    "            left = self.stringify_condition(cond.left)\n",
    "            right = self.stringify_condition(cond.right)\n",
    "            return f\"({left} EQUAL EQUAL {right})\"\n",
    "        elif isinstance(cond, Identifier):\n",
    "            return cond.name\n",
    "        elif isinstance(cond, IntConst):\n",
    "            return cond.value\n",
    "        elif isinstance(cond, Cond):\n",
    "            # Ternary: cond.cond ? cond.true_value : cond.false_value\n",
    "            c = self.stringify_condition(cond.cond)\n",
    "            t = self.stringify_condition(cond.true_value)\n",
    "            f = self.stringify_condition(cond.false_value)\n",
    "            return f\"({c}) IF TRUE ({t}) OTHERWISE ({f})\"\n",
    "        elif isinstance(cond, Pointer):\n",
    "            base_str = self.stringify_condition(cond.var)  # 'b'\n",
    "            ptr_str = self.stringify_condition(cond.ptr)  # 'i'\n",
    "            return f\"{base_str}[{ptr_str}]\"\n",
    "        elif isinstance(cond, Partselect):\n",
    "            # e.g. var = 'b', msb = IntConst('3'), lsb = IntConst('0')\n",
    "            base_str = self.stringify_condition(cond.var)  # \"b\"\n",
    "            msb_str = self.stringify_condition(cond.msb)  # \"3\"\n",
    "            lsb_str = self.stringify_condition(cond.lsb)  # \"0\"\n",
    "            return f\"{base_str}[{msb_str}:{lsb_str}]\"\n",
    "        elif isinstance(cond, Concat):\n",
    "            # Extract and stringify each element in the concatenation\n",
    "            elements = [self.stringify_condition(part) for part in cond.children()]\n",
    "            # Join elements with commas and wrap them in curly braces\n",
    "            return \"{\" + \", \".join(elements) + \"}\"\n",
    "\n",
    "        else:\n",
    "            # Fallback\n",
    "            return str(cond)\n",
    "    def visit_ForStatement(self, node):\n",
    "        \"\"\"\n",
    "        Extract details of a for loop.\n",
    "        \"\"\"\n",
    "        loop_info = {\n",
    "            \"type\": \"for_loop\",\n",
    "            \"initialization\": self.extract_assignments(node.children()[0]),\n",
    "            \"condition\": self.stringify_condition(node.cond),\n",
    "            \"increment\": self.extract_assignments(node.children()[2]),\n",
    "            \"body\": self.extract_logic(node.children()[3]),\n",
    "        }\n",
    "    \n",
    "        # Add the loop_info to an appropriate collection or append it to the block where it's encountered\n",
    "        return loop_info\n",
    "    \n",
    "\n",
    "def parse_verilog_always_blocks(filename):\n",
    "    \"\"\"\n",
    "    Parse the given Verilog file, collect all always blocks,\n",
    "    and return them as a list of structured JSON-like dicts.\n",
    "    \"\"\"\n",
    "    ast, _ = parse(filename)\n",
    "    extractor = VerilogInfoExtractor()\n",
    "    extractor.visit(ast)\n",
    "    return {\n",
    "        \"always_blocks\": extractor.always_blocks,\n",
    "        \"assign_statements\": extractor.assign_statements,\n",
    "        \"module_instantiations\": extractor.module_instantiations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49890214-6393-4580-a02c-ce27a02d9344",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7c543192-1876-4c78-814d-b6c901cebf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model name\n",
    "def preprocess_json_for_embedding(json_data):\n",
    "    chunks = []\n",
    "\n",
    "    # Helper function to process assignments\n",
    "    def process_assignments(assignments, trigger=\"\", condition=\"\"):\n",
    "        for assignment in assignments:\n",
    "            chunk = json.dumps({\n",
    "                \"trigger\": trigger,\n",
    "                \"condition\": condition,\n",
    "                \"assignment\": assignment\n",
    "            })\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    # Helper function to process for-loops recursively\n",
    "    def process_for_loop(for_loop, trigger=\"\"):\n",
    "        # Extract for-loop details\n",
    "        initialization = \"; \".join(\n",
    "            f\"{init['lhs']} = {init['rhs']}\" for init in for_loop.get(\"initialization\", [])\n",
    "        )\n",
    "        condition = for_loop.get(\"condition\", \"None\")\n",
    "        increment = \"; \".join(\n",
    "            f\"{inc['lhs']} = {inc['rhs']}\" for inc in for_loop.get(\"increment\", [])\n",
    "        )\n",
    "\n",
    "        # Create a chunk summarizing the for-loop\n",
    "        loop_summary = json.dumps({\n",
    "            \"trigger\": trigger,\n",
    "            \"type\": \"for_loop\",\n",
    "            \"initialization\": initialization,\n",
    "            \"condition\": condition,\n",
    "            \"increment\": increment\n",
    "        })\n",
    "        chunks.append(loop_summary)\n",
    "\n",
    "        # Process the body of the loop\n",
    "        for body_element in for_loop.get(\"body\", []):\n",
    "            if isinstance(body_element, list):  # Body is a nested list\n",
    "                for element in body_element:\n",
    "                    if \"condition\" in element:  # Handle if-like structures\n",
    "                        nested_condition = element.get(\"condition\", \"None\")\n",
    "                        assignments = element.get(\"assignments\", [])\n",
    "                        process_assignments(assignments, trigger, nested_condition)\n",
    "                    elif element.get(\"type\") == \"for_loop\":  # Handle nested for-loops\n",
    "                        process_for_loop(element, trigger)\n",
    "    def process_case_statement(case_statement, trigger=\"\"):\n",
    "        case_expr = case_statement.get(\"case_expr\", \"Unknown Expression\")\n",
    "    \n",
    "        for case in case_statement.get(\"cases\", []):\n",
    "            conditions = \" | \".join(case.get(\"conditions\", []))\n",
    "            for statement in case.get(\"statements\", []):\n",
    "                if statement.get(\"type\") == \"assignment\":\n",
    "                    assignment = {\n",
    "                        \"lhs\": statement.get(\"lhs\", \"Unknown LHS\"),\n",
    "                        \"rhs\": statement.get(\"rhs\", \"Unknown RHS\")\n",
    "                    }\n",
    "                    chunk = json.dumps({\n",
    "                        \"trigger\": trigger,\n",
    "                        \"condition\": conditions,\n",
    "                        \"case_expr\": case_expr,\n",
    "                        \"assignment\": assignment\n",
    "                    })\n",
    "                    chunks.append(chunk)\n",
    "\n",
    "    # Process always blocks\n",
    "    for always_block in json_data.get(\"always_blocks\", []):\n",
    "        triggers = \", \".join(always_block.get(\"triggers\", []))\n",
    "        for logic in always_block.get(\"logic\", []):\n",
    "            if logic.get(\"type\") == \"for_loop\":\n",
    "                process_for_loop(logic, trigger=triggers)\n",
    "            elif logic.get(\"type\") == \"case_statement\":\n",
    "                process_case_statement(logic, trigger=triggers)\n",
    "            elif \"condition\" in logic and \"assignments\" in logic:\n",
    "                condition = logic.get(\"condition\", \"None\")\n",
    "                assignments = logic.get(\"assignments\", [])\n",
    "                process_assignments(assignments, triggers, condition)\n",
    "\n",
    "    # Process assign statements\n",
    "    for assign in json_data.get(\"assign_statements\", []):\n",
    "        chunk = json.dumps(assign)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Process module instantiations\n",
    "    for module_inst in json_data.get(\"module_instantiations\", []):\n",
    "        chunk = json.dumps(module_inst)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "def embed_text(chunks, model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Embed the text chunks using the HuggingFace embedding model.\n",
    "    \"\"\"\n",
    "    # Initialize embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # Create a vector store using DocArrayInMemorySearch\n",
    "    vector_store = DocArrayInMemorySearch.from_texts(chunks, embedding=embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ced36e44-c659-443d-b5fe-e6e387b586b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vector store\n",
    "import json\n",
    "import difflib\n",
    "\n",
    "def find_fuzzy_trace_path(filepath, signal_query, module_query, cutoff=0.6):\n",
    "    with open(filepath, 'r') as f:\n",
    "        signals = json.load(f)\n",
    "    # Just load the whole JSON to signals\n",
    "    # Build lookup keys from (signal, module) pairs\n",
    "    key_map = { (entry.get(\"signal\", \"\"), entry.get(\"module\", \"\")): entry for entry in signals }\n",
    "    keys = list(key_map.keys())\n",
    "\n",
    "    # Create formatted strings for fuzzy matching\n",
    "    formatted_keys = [f\"{sig}::{mod}\" for sig, mod in keys]\n",
    "    query_string = f\"{signal_query}::{module_query}\"\n",
    "\n",
    "    matches = difflib.get_close_matches(query_string, formatted_keys, n=1, cutoff=cutoff)\n",
    "\n",
    "    if matches:\n",
    "        matched_sig, matched_mod = matches[0].split(\"::\")\n",
    "        matched_entry = key_map.get((matched_sig, matched_mod))\n",
    "        return matched_entry.get(\"trace_path\", [])\n",
    "\n",
    "    return \"Trace path not found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7c54a2c1-f5f4-48e9-9690-0fcde6c990f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_trace_json(node, depth=0, visited=None, max_depth=4, history=None, flatten_parts=None, decoder_stage=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if history is None:\n",
    "        history = []\n",
    "    if flatten_parts is None:\n",
    "        flatten_parts = []\n",
    "    if decoder_stage is None:\n",
    "        decoder_stage = {\"found\": False}  # Use a dict to hold state\n",
    "    if decoder_stage[\"found\"]:\n",
    "        return history, \"DEPTH_LIMIT\"    \n",
    "    if node is None or depth > max_depth:\n",
    "        # print(\"  \" * depth + f\"Max depth {max_depth} reached.\")\n",
    "        return history, \"DEPTH_LIMIT\"\n",
    "    indent = \"  \" * depth\n",
    "        \n",
    "    for i, child in enumerate(node.children()):\n",
    "        if node.__class__.__name__ == \"DFBranch\" and i == 1:\n",
    "            # print(f\"{indent} Skipping tracing of condition: {child.tocode()}\")\n",
    "            continue\n",
    "        if hasattr(child, \"tocode\"):\n",
    "            recursive_trace_json(child, depth + 1, visited, max_depth, history, flatten_parts, decoder_stage=decoder_stage)\n",
    "        \n",
    "    \n",
    "    # Leaf node reached    \n",
    "    if len(node.children()) == 0 and hasattr(node, \"name\"):\n",
    "        leaf_signal = str(node.name)\n",
    "        if leaf_signal in visited:\n",
    "            return history, None\n",
    "        visited.add(leaf_signal)\n",
    "\n",
    "        # Check binddict for drivers of this leaf signal\n",
    "        for k in binddict:\n",
    "            if str(k).endswith(leaf_signal) and not \"atomic\" in str(k) and not \"fpu\" in str(k):\n",
    "                module = str(k).split('.')[-2]\n",
    "                for expr in binddict[k]:\n",
    "                    if expr.tree:\n",
    "                        expr_str = expr.tree.tocode()\n",
    "                        if module == \"mor1kx_decode\":\n",
    "                            decoder_stage[\"found\"] = True  # Update mutable container\n",
    "                        # Add to structured history\n",
    "                        history.append({\n",
    "                            \"module\": module,\n",
    "                            \"assign\": f\"{leaf_signal} = {expr_str}\"\n",
    "                        })\n",
    "                        flatten_parts.append((leaf_signal, expr_str))\n",
    "                        # Recurse on the newly found expression tree\n",
    "                        recursive_trace_json(expr.tree, depth + 1, visited, max_depth, history, decoder_stage=decoder_stage)\n",
    "                        \n",
    "                    else:\n",
    "                        history.append({\n",
    "                            \"module\": module,\n",
    "                            \"port_map\": f\"{leaf_signal} is a port (no logic in this module)\"\n",
    "                        })\n",
    "                    \n",
    "    return history, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8840ce27-4db4-48ef-a73c-eab6016ecac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyverilog.vparser.parser import parse\n",
    "from pyverilog.dataflow.dataflow_analyzer import VerilogDataflowAnalyzer\n",
    "from pyverilog.utils.scope import ScopeChain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b692786f-ef77-4755-8b07-51dc6988fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load the JSON file\n",
    "# with open(\"HIGH_LEVEL_EVENTS_CTRL.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# data = trace\n",
    "top_module = \"mor1kx_cpu_cappuccino\"\n",
    "\n",
    "# Simplify signal names: remove top-level prefixes and collapse hierarchy\n",
    "def simplify_signal(signal):\n",
    "    signal = signal.replace(top_module + \"_\", \"\")\n",
    "    signal = signal.replace(top_module + \".\", \"\")\n",
    "    parts = signal.split(\".\")\n",
    "    if len(parts) > 1:\n",
    "        last_module = parts[-2]\n",
    "        signal_name = parts[-1]\n",
    "        return f\"{last_module}.{signal_name}\"\n",
    "    return signal\n",
    "\n",
    "# Replace logical operators in expressions\n",
    "def replace_logical_operators(expr):\n",
    "    expr = expr.replace(\"|\", \" OR \")\n",
    "    expr = expr.replace(\"&\", \" AND \")\n",
    "    expr = expr.replace(\"~\", \" NOT \")\n",
    "    expr = expr.replace(\"!\", \" NOT \")\n",
    "    return expr\n",
    "\n",
    "# Simplify each token in the RHS expression (handles OR, AND, NOT)\n",
    "def simplify_rhs_expression(rhs):\n",
    "    tokens = re.split(r'(\\bOR\\b|\\bAND\\b|\\bNOT\\b|\\(|\\))', rhs)\n",
    "    simplified_tokens = [\n",
    "        simplify_signal(token.strip()) if re.match(r'\\w[\\w\\d_.]*', token.strip()) else token\n",
    "        for token in tokens\n",
    "    ]\n",
    "    return ' '.join(filter(None, simplified_tokens))\n",
    "def strip_outer_parentheses(expr):\n",
    "    expr = expr.strip()\n",
    "    while expr.startswith('(') and expr.endswith(')'):\n",
    "        # Check if parentheses are balanced\n",
    "        level = 0\n",
    "        balanced = True\n",
    "        for idx, char in enumerate(expr):\n",
    "            if char == '(':\n",
    "                level += 1\n",
    "            elif char == ')':\n",
    "                level -= 1\n",
    "            if level == 0 and idx != len(expr)-1:\n",
    "                balanced = False\n",
    "                break\n",
    "        if balanced:\n",
    "            expr = expr[1:-1].strip()\n",
    "        else:\n",
    "            break\n",
    "    return expr\n",
    "\n",
    "# Recursively format ternary expressions into IF-THEN-ELSE blocks\n",
    "def format_ternary(expr, indent=0):\n",
    "    expr = strip_outer_parentheses(expr)  # Add this!\n",
    "    spaces = \"  \" * indent\n",
    "\n",
    "    # Base case: if no ternary detected\n",
    "    if '?' not in expr or ':' not in expr:\n",
    "        return f\"{spaces}{simplify_rhs_expression(expr)}\"\n",
    "\n",
    "    # Track parentheses nesting, find top-level '?' and matching ':'\n",
    "    level = 0\n",
    "    qmark_idx, colon_idx = None, None\n",
    "    for idx, char in enumerate(expr):\n",
    "        if char == '(':\n",
    "            level += 1\n",
    "        elif char == ')':\n",
    "            level -= 1\n",
    "        elif char == '?' and level == 0 and qmark_idx is None:\n",
    "            qmark_idx = idx\n",
    "        elif char == ':' and level == 0 and qmark_idx is not None:\n",
    "            colon_idx = idx\n",
    "            break\n",
    "\n",
    "    # If no top-level '?' and ':' found, fallback to base case\n",
    "    if qmark_idx is None or colon_idx is None:\n",
    "        return f\"{spaces}{simplify_rhs_expression(expr)}\"\n",
    "\n",
    "    # Extract condition, true_expr, false_expr\n",
    "    condition = expr[:qmark_idx].strip(\"() \")\n",
    "    true_expr = expr[qmark_idx+1:colon_idx].strip()\n",
    "    false_expr = expr[colon_idx+1:].strip()\n",
    "\n",
    "    # Format recursively\n",
    "    return (\n",
    "        f\"{spaces}IF {simplify_signal(condition)} == TRUE THEN\\n\"\n",
    "        f\"{format_ternary(true_expr, indent+1)}\\n\"\n",
    "        f\"{spaces}ELSE\\n\"\n",
    "        f\"{format_ternary(false_expr, indent+1)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Split true/false parts in a ternary expression\n",
    "def split_ternary_parts(remaining):\n",
    "    level, idx = 0, None\n",
    "    for i, char in enumerate(remaining):\n",
    "        if char == '(':\n",
    "            level += 1\n",
    "        elif char == ')':\n",
    "            level -= 1\n",
    "        elif char == ':' and level == 0:\n",
    "            idx = i\n",
    "            break\n",
    "    true_expr = remaining[:idx].strip()\n",
    "    false_expr = remaining[idx+1:].strip()\n",
    "    return true_expr, false_expr\n",
    "\n",
    "# Process and output\n",
    "# target_signal = \"ctrl_flag_clear\"\n",
    "# target_module = \"mor1kx_ctrl_cappuccino\"\n",
    "\n",
    "# for entry in data:\n",
    "#     if entry[\"signal\"] == target_signal and entry[\"module\"] == target_module:\n",
    "#         print(f\"Net name: {target_signal}\")\n",
    "#         print(f\"Found in module: atomic_flag_clear\")\n",
    "#         print(f\"Top level module: {top_module}\")\n",
    "#         print(\"Logical Tracing tree:\")\n",
    "# for step in entry[\"trace_path\"]:\n",
    "def generate_tracing_tree(data):\n",
    "    output = []  # Collect all lines here\n",
    "    for step in data:\n",
    "        module = simplify_signal(step[\"module\"])    \n",
    "        assign_expr = step[\"assign\"]\n",
    "        lhs, rhs = map(str.strip, assign_expr.split('=', 1))\n",
    "        lhs = simplify_signal(lhs)\n",
    "        rhs = replace_logical_operators(rhs)\n",
    "        if '?' in rhs and ':' in rhs:\n",
    "            formatted_rhs = format_ternary(rhs)\n",
    "            formatted_rhs = replace_logical_operators(formatted_rhs)  # Replace AFTER ternary formatting\n",
    "            # print(f\"In module {module},\\n{lhs} =\\n{formatted_rhs}\")\n",
    "            output.append(f\"In module {module},\\n{lhs} =\\n{formatted_rhs}\")\n",
    "\n",
    "        else:\n",
    "            simplified_rhs = simplify_rhs_expression(rhs)\n",
    "            # print(f\"In module {module},\\n{lhs} = {simplified_rhs}\")\n",
    "            output.append(f\"In module {module},\\n{lhs} = {simplified_rhs}\")\n",
    "    return \"\\n\".join(output)  # Combine all lines into a single string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd477d5a-a4e4-4700-8c97-6b295646989e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2f390-cf3a-49ea-8b14-5d1676e62ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e214cde2-d7f3-4ef2-a80a-96d8c35cd137",
   "metadata": {},
   "source": [
    "### semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22735e97-cba1-49bb-b32f-c1b827e2e322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1bc94dc7-d1b4-44cd-8de3-6d8f7eb6279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def semantic_search_with_history(vector_store=None, queries, hiers, target_matching, llm):\n",
    "import time\n",
    "import re\n",
    "pattern = re.compile(r\"'total_tokens':\\s*(\\d+)\")\n",
    "def semantic_search_with_history(queries, hiers, llm, filepath, module, file_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform semantic search to retrieve relevant specification chunks for a query.\n",
    "    \"\"\"\n",
    "\n",
    "    # def get_relevant_docs(query):\n",
    "    #     retrieved_docs = vector_store.similarity_search(query, k=target_matching)\n",
    "    #     # print(\"RD: \", retrieved_docs)\n",
    "    #     if not retrieved_docs:\n",
    "    #         return None\n",
    "    #     # Filter documents to only include those containing \"ibus_adr\"\n",
    "    #     filtered_docs = [\n",
    "    #         doc for doc in retrieved_docs if query in doc.page_content\n",
    "    #     ]\n",
    "    #     if not filtered_docs:\n",
    "    #         print(\"No logical expressions matched with the query net => \", query)\n",
    "    #         return None\n",
    "    #     # Combine all retrieved document contents into a single context\n",
    "    #     combined_docs = \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "    #     # print(\"CD: \", combined_docs)\n",
    "    #     return combined_docs\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        FINAL_TEMPLATE = \"\"\"\n",
    "        1. Role Assignment: \n",
    "        • You are an expert in computer architecture, specializing in abstracting hardware signal traces into high-level \n",
    "        architectural behaviors related to instruction execution.\n",
    "    \n",
    "        2. Context:\n",
    "        • The trace comes from an OpenRISC-1000 implementation, but your output must be ISA- and core-agnostic (valid for \n",
    "        OR1K, RISC-V, ARM, x86).\n",
    "        • Architectural *event* = a high-level phenomenon visible to ISA software (e.g. “pipeline stall on operand hazard”).\n",
    "        • Ignore micro-architectural configuration options except In-Order execution (e.g. Atomic).\n",
    "        3. Purpose:\n",
    "        • Translate an OpenRISC-1000 RTL trace into an architecture-agnostic event description that software engineers can \n",
    "        use to craft C test programs which will trigger that architecture-agnostic event.\n",
    "        4. Rule:\n",
    "        ──────────────────────── CONTRACT START ──────────────────────\n",
    "        ANALYSIS LADDER  (follow in order, no omissions)\n",
    "        [L1] **Signal Normalization**  – simplify expression, fold constants, strip feature macros, trim widths \n",
    "        [L2] **Dependency Chase**      – list root signals that directly gate the target signal and their boolean relation  \n",
    "        [L3] **Micro-Arch Role**       – describe what this signal is trying to achieve in the processor not how the implementation wires it.  \n",
    "        [L4] **Architectural Event**   – translate L3 and net's purpose into a phrase in ISA-level terms\n",
    "        [L5] **Test-Stimulus Hints**   – How software (C code with operations like arithmatic, memory access, flag-setting etc.) can provoke L4. \n",
    "        [L6] **Grouping Instructions** – Group related instructions into categories that influence this signal.\n",
    "        \n",
    "        MANDATORY QUESTIONS TO ANSWER\n",
    "        Net: <signal_name>\n",
    "        [Q1] High-Level Event: <one short sentence>\n",
    "        [Q2] Logical Summary & Reasoning:<combining L2+L3>\n",
    "        [Q3] Test-Stimulus Guidance:<L5>\n",
    "        [Q4] Instruction Categories:<The types of relevant instructions that influence this signal and can be grouped into categories>\n",
    "        GUIDELINES\n",
    "        • Do **not** mention RTL module names, internal signal names (other than the one in “Net:”)\n",
    "        • Identify the first pipeline stage where the net can influence architectural state\n",
    "          and work from that perspective.  \n",
    "        • Be as detailed or as brief as you feel appropriate—no word-count limits apply.\n",
    "        ──────────────────────── CONTRACT END ────────────────────────\n",
    "        5. Output exactly these five labelled sections. Anything other than this 5 labels - submission is rejected\n",
    "        Net: <signal_name>\n",
    "        \n",
    "        High-Level Event: <text>\n",
    "        \n",
    "        Logical Summary & Reasoning:<text>\n",
    "        \n",
    "        Test-Stimulus Guidance:<text>\n",
    "        \n",
    "        Instruction Categories:<text>\n",
    "    \n",
    "        \n",
    "        Again **DO NOT disclose and low level RTL details**.Breaking any clause and this rule ⇒ the submission is rejected automatically.\n",
    "        \"\"\"\n",
    "        class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "            \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "    \n",
    "            messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    \n",
    "            def add_message(self, message: BaseMessage) -> None:\n",
    "                \"\"\"Add a self-created message to the store\"\"\"\n",
    "                self.messages.append(message)\n",
    "    \n",
    "            def clear(self) -> None:\n",
    "                self.messages = []\n",
    "    \n",
    "        store = {}\n",
    "    \n",
    "        def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "            if (user_id, conversation_id) not in store:\n",
    "                store[(user_id, conversation_id)] = InMemoryHistory()\n",
    "            return store[(user_id, conversation_id)]\n",
    "    \n",
    "    \n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"{instruction}\\n\\n\"),\n",
    "                # MessagesPlaceholder(variable_name=\"history\"), # changed by MHS 03/06\n",
    "                # (\"ai\", \"{last_message}\"), # inject last message here\n",
    "                (\"human\", \"Net name: {question}\\n\\nModule Hierarchy:{hier}\\n\\nJSON text fomat:\\n{doc}\\n\\n\"),\n",
    "            ]\n",
    "        )\n",
    "        # def debug_prompt(input_data):\n",
    "        #     formatted_prompt = prompt.invoke(input_data)\n",
    "        #     # print(\"Prompt Sent to LLM:\\n\", formatted_prompt)\n",
    "        #     # print(\"\\n================Sent=================\")\n",
    "        #     return formatted_prompt\n",
    "    \n",
    "        chain = prompt | llm\n",
    "    \n",
    "        with_message_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history=get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "            history_factory_config=[\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"user_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"User ID\",\n",
    "                    description=\"Unique identifier for the user.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "                ConfigurableFieldSpec(\n",
    "                    id=\"conversation_id\",\n",
    "                    annotation=str,\n",
    "                    name=\"Conversation ID\",\n",
    "                    description=\"Unique identifier for the conversation.\",\n",
    "                    default=\"\",\n",
    "                    is_shared=True,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        results = {}\n",
    "        query = queries[0]\n",
    "        hier = hiers[0]\n",
    "        user_id = \"050725\"\n",
    "        conversation_id = \"050725_cid\"\n",
    "        trace = find_fuzzy_trace_path(filepath, queries[0], module)\n",
    "        doc = generate_tracing_tree(trace)\n",
    "        # print(\"============= DOC ================\")\n",
    "        # print(doc)\n",
    "        # print(\"==================================\")\n",
    "        #doc = get_relevant_docs_json(queries[0])\n",
    "        last_message = \"\"\n",
    "        result = with_message_history.invoke(\n",
    "            #{\"instruction\": SUB_INS, \"doc\": doc, \"question\": query, \"hier\": hier, \"last_message\": last_message},\n",
    "            {\"instruction\": FINAL_TEMPLATE, \"doc\": doc, \"question\": query, \"hier\": hier},\n",
    "            config={\n",
    "                \"configurable\": {\"user_id\": user_id, \"conversation_id\": conversation_id}\n",
    "            },\n",
    "        )\n",
    "        results[query] = [result.content]\n",
    "        results[query].append(result.usage_metadata)\n",
    "        print(\"High level event:\\n\", result.content)\n",
    "        print(\"Token Count:\\n\", result.usage_metadata)\n",
    "        file.write(result.content + \"\\n\\n\")\n",
    "        total = 0\n",
    "        match = pattern.search(str(result.usage_metadata))\n",
    "        if match:\n",
    "            total += int(match.group(1))\n",
    "            \n",
    "        for i in range(1, len(queries)):\n",
    "            time.sleep(10)\n",
    "            query = queries[i]\n",
    "            hier = hiers[i]\n",
    "            trace = find_fuzzy_trace_path(filepath, queries[i], module)\n",
    "            doc = generate_tracing_tree(trace)\n",
    "            # doc = get_relevant_docs_json(queries[i])\n",
    "            history = get_session_history(user_id, conversation_id)\n",
    "            last_message = history.messages[-1].content if history.messages else \"\"\n",
    "            last_message = \"\"\n",
    "            # print(\"Last messages: ======> \", last_message)\n",
    "            # formatted_prompt = prompt.format_prompt(\n",
    "            #     instruction=SORT_INS,\n",
    "            #     doc=doc,\n",
    "            #     hier=hier,\n",
    "            #     question=query,\n",
    "            #     last_message=last_message\n",
    "            # )\n",
    "            # print(formatted_prompt.to_messages())  # See each message role/content\n",
    "            result = with_message_history.invoke(\n",
    "                {\"instruction\": FINAL_TEMPLATE,\"doc\": doc, \"question\": query, \"hier\": hier},\n",
    "                #{\"instruction\": SUB_INS,\"doc\": doc, \"question\": query, \"hier\": hier, \"last_message\": last_message},\n",
    "                config={\n",
    "                    \"configurable\": {\"user_id\": user_id, \"conversation_id\": conversation_id}\n",
    "                },\n",
    "            )\n",
    "            results[query] = [result.content]\n",
    "            results[query].append(result.usage_metadata)\n",
    "            print(\"High level event:\\n\", result.content)\n",
    "            print(\"Token Count:\\n\", result.usage_metadata)\n",
    "            file.write(result.content + \"\\n\\n\")\n",
    "            match = pattern.search(str(result.usage_metadata))\n",
    "            if match:\n",
    "                total += int(match.group(1))\n",
    "        \n",
    "    print(\"Toal Token Count:\\n\", total)\n",
    "    return results, store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136eb61-13fc-4c4b-935a-b91fe7772d63",
   "metadata": {},
   "source": [
    "### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7d7f561a-6f8b-4411-bf37-159b1a447c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "WARNING: 183 shift/reduce conflicts\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "verilog_file = []\n",
    "verilog_files = [\n",
    "    \"mor1kx-defines.v\",\n",
    "    \"mor1kx-sprs.v\",\n",
    "    \"mor1kx_fetch_cappuccino.v\",  # Top-level module\n",
    "    \"mor1kx_icache.v\",\n",
    "    \"mor1kx_immu.v\",\n",
    "    \"mor1kx_simple_dpram_sclk.v\",\n",
    "    \"mor1kx_true_dpram_sclk.v\",\n",
    "    \"mor1kx_store_buffer.v\",\n",
    "    \"mor1kx_cache_lru.v\",\n",
    "]\n",
    "verilog_files_decoder = [\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_decode.v\"\n",
    "    ]\n",
    "verilog_files_ctrl = [\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_ctrl_cappuccino.v\"\n",
    "]\n",
    "\n",
    "filelist = [\n",
    "    # \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx-defines.v\",\n",
    "    # \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx-sprs.v\",\n",
    "    # \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_utils.vh\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_cpu_cappuccino.v\", # Top-level module\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_fetch_cappuccino.v\",  \n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_icache.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_dcache.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_immu.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_dmmu.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_simple_dpram_sclk.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_true_dpram_sclk.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_store_buffer.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_cache_lru.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_decode.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_decode_execute_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_branch_prediction.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_branch_predictor_simple.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_execute_alu.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_lsu_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_wb_mux_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_rf_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_execute_ctrl_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_ctrl_cappuccino.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_cfgrs.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_pic.v\",\n",
    "    \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification/mor1kx_verilogs/mor1kx_ticktimer.v\",\n",
    "    \n",
    "]\n",
    "topmodule = 'mor1kx_cpu_cappuccino'\n",
    "# === STEP 1: Parse and Build Analyzer ===\n",
    "analyzer = VerilogDataflowAnalyzer(filelist, topmodule)\n",
    "analyzer.generate()\n",
    "terms = analyzer.getTerms()\n",
    "binddict = analyzer.getBinddict()\n",
    "\n",
    "########### Previous Way #################################\n",
    "# always_blocks = parse_verilog_always_blocks(verilog_files_ctrl)\n",
    "# # Specify the output JSON file path\n",
    "# output_file = \"always_blocks_ctrl.json\"\n",
    "# # print(json.dumps(always_blocks, indent=2))\n",
    "# # Write the data to a JSON file\n",
    "# with open(output_file, \"w\") as json_file:\n",
    "#     json.dump(always_blocks, json_file, indent=2)\n",
    "\n",
    "# print(f\"Always blocks saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f15a1-7ce7-4100-a261-1e9320134d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "# with open(output_file, \"r\") as file:\n",
    "#     verilog_json = json.load(file)  # Load the JSON content as a Python dictionary\n",
    "# chunks = preprocess_json_for_embedding(verilog_json)\n",
    "# # Write each chunk on a separate line\n",
    "\n",
    "# with open(\"always_blocks_chunks.txt\", \"w\") as file:\n",
    "#     for chunk in chunks:\n",
    "#         file.write(chunk + \"\\n\")\n",
    "# vector_store = embed_text(chunks, model_name=EMBEDDING_MODEL_NAME)\n",
    "for key in binddict.keys():\n",
    "    if \"mor1kx_lsu_cappuccino\" in str(key):\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "425ba8f5-35a7-4fe0-ace8-75200a26bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Read the file and parse lines\n",
    "with open('/home/m588h354/projects/autophasew/openrisc/src/vcd_texts/lsu.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract the net names (3rd column)\n",
    "net_names = [line.strip().split(None, 2)[2] for line in lines if line.strip()]\n",
    "\n",
    "# Count occurrences\n",
    "net_counts = Counter(net_names)\n",
    "\n",
    "# Sort nets alphabetically\n",
    "sorted_nets = sorted(net_counts.items())\n",
    "\n",
    "# Write to output file\n",
    "with open('/home/m588h354/projects/Rare_net_analysis-repo/event_identification/all_nets_lsu.txt', 'w') as f:\n",
    "    for net, count in sorted_nets:\n",
    "        f.write(f\"{net} {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f6fbdd54-0d8d-4cdd-9f7d-517ffedfdc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpu_err_o', 'cpu_req_i', 'cpu_we_i', 'current_lru', 'current_lru_history', 'dc_access_i', 'dc_dbus_err_i', 'dc_enable_i', 'hit', 'invalidate', 'invalidate_ack', 'invalidate_adr', 'next_lru_history', 'next_refill_adr', 'read', 'refill', 'refill_adr_i', 'refill_allowed_i', 'refill_dat_i', 'refill_done', 'refill_hit', 'refill_req_o', 'refill_valid', 'refill_we_i', 'snoop_adr_i', 'snoop_check', 'snoop_check_way_match(0)', 'snoop_check_way_match(1)', 'snoop_check_way_tag(0)', 'snoop_check_way_tag(1)', 'snoop_check_way_valid(0)']\n",
      "['TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache', 'TOP->orpsoc_top->gencpu->mor1kx0->mor1kx_cpu->cappuccino->mor1kx_cpu->mor1kx_lsu_cappuccino->dcache_gen->mor1kx_dcache']\n"
     ]
    }
   ],
   "source": [
    "def extract_net_names(file_path, start_line, end_line):\n",
    "    net_list = []\n",
    "    module_hier = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for i, line in enumerate(file, start=1):\n",
    "            if start_line <= i <= end_line:\n",
    "                parts = line.split()\n",
    "                if parts:\n",
    "                    net = parts[0]\n",
    "                    net_parts = net.split(\".\")\n",
    "                    module = \".\".join(net_parts[:-1])\n",
    "                    module = module.replace(\".\", \"->\")\n",
    "                    net_name = net.split(\".\")[-1].split(\"[\")[0]\n",
    "                    net_list.append(net_name)\n",
    "                    module_hier.append(module)\n",
    "            elif i > end_line:\n",
    "                break\n",
    "    \n",
    "    return net_list, module_hier\n",
    "nets, module_hier = extract_net_names(\"./all_nets_lsu.txt\", 60, 90)\n",
    "#nets, module_hier = extract_net_names(\"./fetch_nets_rareness.txt\", 1, 2)\n",
    "print(nets)\n",
    "print(module_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "059208c6-98ff-4890-b39a-735c3f4bceec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.cpu_err_o\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.cpu_req_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.cpu_we_i\n",
      "Net current_lru is not in the AST!\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.current_lru_history\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.dc_access_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.dc_dbus_err_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.dc_enable_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.hit\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.invalidate\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.invalidate_ack\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.invalidate_adr\n",
      "Net next_lru_history is not in the AST!\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.next_refill_adr\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.read\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_adr_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_allowed_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_dat_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_done\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_hit\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_req_o\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_valid\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.refill_we_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.snoop_adr_i\n",
      "Matched: mor1kx_cpu_cappuccino.mor1kx_lsu_cappuccino.md_generate17.ge_if21.dcache_gen.mor1kx_dcache.snoop_check\n",
      "Net snoop_check_way_match(0) is not in the AST!\n",
      "Net snoop_check_way_match(1) is not in the AST!\n",
      "Net snoop_check_way_tag(0) is not in the AST!\n",
      "Net snoop_check_way_tag(1) is not in the AST!\n",
      "Net snoop_check_way_valid(0) is not in the AST!\n",
      "JSON written to HIGH_LEVEL_EVENTS_LSU.json\n"
     ]
    }
   ],
   "source": [
    "target_signal_list = []\n",
    "nets_to_remove = []\n",
    "def find_signal_key(target_signal, module):\n",
    "    # print(f\"T_S: {target_signal}, and MODULE {module}\")\n",
    "    for key in binddict.keys():\n",
    "        # if target_signal in str(key).split('.')[-1]:\n",
    "        #     print(f\"================> : {str(key)}\")\n",
    "        if target_signal == str(key).split('.')[-1] and str(key).split('.')[1] == module:\n",
    "            print(f\"Matched: {str(key)}\")\n",
    "            return key\n",
    "    return None\n",
    "full_output = []\n",
    "# signal_in_module = 'mor1kx_ctrl_cappuccino'\n",
    "signal_in_module = 'mor1kx_lsu_cappuccino'\n",
    "# net_count = \n",
    "with open(\"HIGH_LEVEL_EVENTS_LSU.json\", \"w\") as f:\n",
    "    # Signal to trace\n",
    "    for net in nets:\n",
    "        signal_to_trace = net\n",
    "        scoped_key = find_signal_key(signal_to_trace, signal_in_module)\n",
    "        if scoped_key is None:\n",
    "            print(f\"Net {signal_to_trace} is not in the AST!\")\n",
    "            nets_to_remove.append(signal_to_trace)\n",
    "        else:\n",
    "            i = scoped_key\n",
    "            # for i in scoped_key:\n",
    "            tree = binddict[i][0].tree\n",
    "            top_module = str(i).split('.')[-2]\n",
    "            history = [{\n",
    "                \"module\": top_module,\n",
    "                \"assign\": f\"{signal_to_trace} = {tree.tocode()}\"\n",
    "            }]\n",
    "            # print(history)\n",
    "            trace_history, final_expr = recursive_trace_json(tree, max_depth=10, visited=set(), history=history)\n",
    "            full_output.append({\n",
    "                \"signal\": signal_to_trace,\n",
    "                \"module\": top_module,\n",
    "                \"trace_path\": trace_history # it adds as a whole\n",
    "            })\n",
    "    json.dump(full_output, f, indent=2)\n",
    "    print(\"JSON written to HIGH_LEVEL_EVENTS_LSU.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a089ce42-4105-4369-bd2c-64c8878203bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpu_err_o',\n",
       " 'cpu_req_i',\n",
       " 'cpu_we_i',\n",
       " 'current_lru_history',\n",
       " 'dc_access_i',\n",
       " 'dc_dbus_err_i',\n",
       " 'dc_enable_i',\n",
       " 'hit',\n",
       " 'invalidate',\n",
       " 'invalidate_ack',\n",
       " 'invalidate_adr',\n",
       " 'next_refill_adr',\n",
       " 'read',\n",
       " 'refill',\n",
       " 'refill_adr_i',\n",
       " 'refill_allowed_i',\n",
       " 'refill_dat_i',\n",
       " 'refill_done',\n",
       " 'refill_hit',\n",
       " 'refill_req_o',\n",
       " 'refill_valid',\n",
       " 'refill_we_i',\n",
       " 'snoop_adr_i',\n",
       " 'snoop_check']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "filtered_net_list = []\n",
    "for item in nets:\n",
    "    if item not in nets_to_remove:\n",
    "        filtered_net_list.append(item)\n",
    "filtered_net_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "4ea2c224-ff1b-4f7c-add7-40cf6047ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High level event:\n",
      " Net: cpu_err_o\n",
      "\n",
      "High-Level Event: Pipeline stall due to cache error or exception.\n",
      "\n",
      "Logical Summary & Reasoning: The cpu_err_o signal is related to errors or exceptions occurring during the execution of instructions, specifically when accessing the cache. This signal is triggered when the processor encounters an issue while trying to fetch or store data, such as a cache miss, a protection violation, or an error in the cache hierarchy. The root cause of this signal can be attributed to the interaction between the processor's load/store unit and the cache, where the processor is attempting to access a memory location that is not valid or is protected. The boolean relation between the signals involved in this process can be described as a combination of cache access requests, memory protection checks, and error detection mechanisms.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this signal, software engineers can write C test programs that perform memory accesses with varying patterns, such as accessing memory locations that are not aligned, attempting to write to read-only memory, or accessing memory locations that are outside the valid address range. Additionally, tests can be designed to exercise the cache hierarchy by performing multiple loads and stores to the same memory location, or by accessing memory locations that are likely to cause cache misses.\n",
      "\n",
      "Instruction Categories: The types of instructions that can influence this signal include: \n",
      "- Memory access instructions (loads and stores)\n",
      "- Instructions that access protected memory regions (e.g., privileged instructions)\n",
      "- Instructions that perform atomic operations or synchronization primitives\n",
      "- Instructions that access memory locations with specific alignment requirements.\n",
      "Token Count:\n",
      " {'input_tokens': 805, 'output_tokens': 308, 'total_tokens': 1113}\n",
      "High level event:\n",
      " Net: cpu_req_i\n",
      "\n",
      "High-Level Event: Pipeline stall due to cache miss or memory access hazard.\n",
      "\n",
      "Logical Summary & Reasoning: The cpu_req_i signal is related to the processor's load/store unit, and its assertion indicates a request for memory access. This signal is influenced by the address being accessed, and its timing is critical in determining the pipeline's progress. The signal's purpose is to manage the flow of data between the processor and the memory subsystem, ensuring that the pipeline is not stalled unnecessarily. The root cause of this signal's assertion can be attributed to the processor's need to access memory, which may lead to a cache miss or a memory access hazard, resulting in a pipeline stall.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, software engineers can craft C test programs that perform memory-intensive operations, such as loading or storing data in a loop, or accessing memory locations that are likely to cause cache misses. Additionally, using pointers to access memory locations with varying degrees of alignment or using memory-mapped I/O devices can also trigger this event.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal can be grouped into categories such as:\n",
      "- Load instructions (e.g., lw, lb, lh)\n",
      "- Store instructions (e.g., sw, sb, sh)\n",
      "- Memory access instructions with potential for cache misses (e.g., accessing large arrays or data structures)\n",
      "- Instructions that access memory-mapped I/O devices or peripherals.\n",
      "Token Count:\n",
      " {'input_tokens': 805, 'output_tokens': 293, 'total_tokens': 1098}\n",
      "High level event:\n",
      " Net: cpu_we_i\n",
      "\n",
      "High-Level Event: Instruction execution triggers a write operation to the CPU's internal state.\n",
      "\n",
      "Logical Summary & Reasoning: The cpu_we_i signal is related to the write enable operation of the CPU's internal state. This signal is influenced by various factors, including exception handling, feature configuration, and instruction execution. The signal's value is determined by a complex set of conditions, including the state of the CPU's control unit, the type of instruction being executed, and the presence of certain features such as data caching, instruction caching, and overflow handling. The signal's purpose is to control the writing of data to the CPU's internal registers and memory.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the cpu_we_i signal, software engineers can craft C test programs that execute a variety of instructions, including those that trigger exceptions, access memory, and manipulate the CPU's internal state. Specific test cases might include: executing a sequence of arithmetic instructions that trigger an overflow exception; accessing a memory location that is not cached; executing a instruction that sets or clears a flag; and executing a instruction that triggers a context switch. By analyzing the cpu_we_i signal in response to these test cases, engineers can gain insight into the CPU's internal behavior and identify potential issues or optimizations.\n",
      "\n",
      "Instruction Categories: The cpu_we_i signal can be influenced by a wide range of instruction categories, including:\n",
      "* Arithmetic instructions (e.g. add, subtract, multiply)\n",
      "* Load and store instructions (e.g. lw, sw)\n",
      "* Branch instructions (e.g. beq, bne)\n",
      "* Exception-generating instructions (e.g. divide by zero)\n",
      "* Flag-manipulating instructions (e.g. set, clear)\n",
      "* Context-switching instructions (e.g. syscall, interrupt)\n",
      "* Memory-accessing instructions (e.g. lw, sw) that interact with the CPU's caching and memory management systems.\n",
      "Token Count:\n",
      " {'input_tokens': 6342, 'output_tokens': 384, 'total_tokens': 6726}\n",
      "High level event:\n",
      " Net: current_lru_history\n",
      "\n",
      "High-Level Event: Cache line replacement due to least recently used (LRU) policy.\n",
      "\n",
      "Logical Summary & Reasoning: The current_lru_history signal is related to the LRU replacement policy in the cache. The signal is influenced by the number of ways in the cache and the tag width. The LRU policy is used to replace the least recently used cache line when the cache is full. The signal is updated based on the cache access patterns, and it helps to determine which cache line to replace when a new cache line is brought in. The LRU policy is a common technique used in caches to improve performance by reducing the number of cache misses.\n",
      "\n",
      "Test-Stimulus Guidance: To trigger the cache line replacement due to LRU policy, a test program can be written to access a set of cache lines in a specific pattern, such that the cache is filled and then a new cache line is accessed, causing the LRU cache line to be replaced. For example, a program can access a set of cache lines in a loop, and then access a new cache line that is not in the cache, causing the LRU cache line to be replaced. The test program can use a combination of load and store instructions to access the cache lines.\n",
      "\n",
      "Instruction Categories: The instruction categories that influence the current_lru_history signal include:\n",
      "* Load instructions: These instructions access data from the cache, and can cause the LRU cache line to be updated.\n",
      "* Store instructions: These instructions store data in the cache, and can cause the LRU cache line to be updated.\n",
      "* Cache manipulation instructions: These instructions can be used to manipulate the cache, such as flushing the cache or invalidating cache lines.\n",
      "* Memory access instructions: These instructions access memory locations that are mapped to the cache, and can cause the LRU cache line to be updated.\n",
      "Token Count:\n",
      " {'input_tokens': 1763, 'output_tokens': 377, 'total_tokens': 2140}\n",
      "High level event:\n",
      " Net: dc_access_i\n",
      "\n",
      "High-Level Event: Data cache access is initiated.\n",
      "\n",
      "Logical Summary & Reasoning: The dc_access_i signal is related to the data cache access. It is influenced by the control signals from the mor1kx_ctrl_cappuccino module, which handles the control flow of the processor. The signal is dependent on various conditions such as exception handling, checking, and feature enabling. The net is trying to achieve the goal of accessing the data cache, which is a critical component of the processor's memory hierarchy. The signal's purpose is to initiate the data cache access, which is a fundamental operation in the processor's instruction execution pipeline.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the dc_access_i signal, software engineers can write C test programs that perform memory-intensive operations, such as loading and storing data from arrays or structures. The programs can also include instructions that access the data cache, such as load, store, and cache flush instructions. Additionally, the programs can be designed to test the signal's behavior under different conditions, such as exception handling and feature enabling.\n",
      "\n",
      "Instruction Categories: The instructions that influence the dc_access_i signal can be grouped into the following categories:\n",
      "* Load instructions (e.g., lw, ld, ldr)\n",
      "* Store instructions (e.g., sw, st, str)\n",
      "* Cache management instructions (e.g., cache flush, cache invalidate)\n",
      "* Exception handling instructions (e.g., trap, exception return)\n",
      "* Control flow instructions (e.g., branch, jump)\n",
      "These instruction categories can be used to craft C test programs that target the dc_access_i signal and test its behavior under different scenarios.\n",
      "Token Count:\n",
      " {'input_tokens': 6342, 'output_tokens': 332, 'total_tokens': 6674}\n",
      "High level event:\n",
      " Net: dc_dbus_err_i\n",
      "\n",
      "High-Level Event: Data cache bus error occurred during an instruction execution.\n",
      "\n",
      "Logical Summary & Reasoning: The signal dc_dbus_err_i is related to an error that occurs during a data cache access. This error can happen when the processor is trying to access a memory location that is not valid or is protected. The signal is influenced by various factors such as the type of instruction being executed, the state of the processor, and the memory management unit (MMU) settings. The error can be triggered by a variety of instructions, including load and store operations, and can be affected by the presence of certain features such as data caching, MMU, and instruction caching.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this signal, a test program can be written to perform a series of load and store operations to a memory location that is not valid or is protected. The test program can also be designed to exercise the MMU and data caching features of the processor. Additionally, the test program can be written to perform a series of instructions that are likely to cause a data cache bus error, such as loading a value from a memory location and then storing it back to the same location without properly handling the cache coherence.\n",
      "\n",
      "Instruction Categories: The instructions that can influence this signal can be grouped into the following categories:\n",
      "* Load and store operations\n",
      "* Memory management unit (MMU) related instructions\n",
      "* Data caching related instructions\n",
      "* Instruction caching related instructions\n",
      "* Arithmetic and logical operations that access memory locations\n",
      "* Control flow instructions that can affect the execution of load and store operations.\n",
      "Token Count:\n",
      " {'input_tokens': 6344, 'output_tokens': 323, 'total_tokens': 6667}\n",
      "High level event:\n",
      " Net: dc_enable_i\n",
      "\n",
      "High-Level Event: The processor enables data caching for a specific instruction.\n",
      "\n",
      "Logical Summary & Reasoning: The dc_enable_i signal is controlled by the mor1kx_ctrl_cappuccino module, which is responsible for managing the processor's control signals. The signal is set based on various conditions, including the presence of exceptions, the type of instruction being executed, and the state of the processor's registers. When the signal is asserted, it indicates that the processor should enable data caching for the current instruction. This is likely used to improve performance by reducing the number of memory accesses required to execute the instruction.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, a software engineer could write a test program that includes instructions that are likely to trigger data caching, such as load and store operations. The program could also include instructions that modify the processor's registers, such as arithmetic and logical operations, to test how the dc_enable_i signal is affected by different instruction sequences. Additionally, the program could include exception-generating instructions, such as division by zero, to test how the signal is affected by exceptions.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence the dc_enable_i signal include:\n",
      "* Load and store operations\n",
      "* Arithmetic and logical operations\n",
      "* Exception-generating instructions (e.g. division by zero)\n",
      "* Control-flow instructions (e.g. branches, jumps)\n",
      "* Register-modifying instructions (e.g. moves, shifts)\n",
      "These instruction categories can be grouped into two main categories: instructions that access memory (load and store operations) and instructions that modify the processor's state (arithmetic, logical, and control-flow instructions). The dc_enable_i signal is likely to be asserted for instructions that access memory, and de-asserted for instructions that modify the processor's state.\n",
      "Token Count:\n",
      " {'input_tokens': 6342, 'output_tokens': 364, 'total_tokens': 6706}\n",
      "High level event:\n",
      " Net: hit\n",
      "\n",
      "High-Level Event: Cache hit occurs due to a successful data retrieval from the cache.\n",
      "\n",
      "Logical Summary & Reasoning: The hit signal is triggered when the requested data is found in the cache, indicating a successful cache access. This event occurs when the address being accessed matches a valid entry in the cache, and the corresponding data is returned. The cache hit event is a result of the processor's load or store operation, where the memory address is checked against the cache contents, and a match is found, allowing the data to be retrieved directly from the cache.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke a cache hit event, a software engineer can write a test program that repeatedly accesses the same memory location, ensuring that the data is initially loaded into the cache. This can be achieved by using a loop that performs a load operation from a specific memory address, followed by a store operation to the same address, and then another load operation to verify that the data is retrieved from the cache.\n",
      "\n",
      "Instruction Categories: The instructions that influence the hit signal can be grouped into the following categories: \n",
      "- Load instructions (e.g., load word, load byte) that access data from memory locations.\n",
      "- Store instructions (e.g., store word, store byte) that write data to memory locations.\n",
      "- Data movement instructions (e.g., move, load immediate) that can also affect cache contents.\n",
      "- Control flow instructions (e.g., jump, branch) that can change the program's execution path and affect cache usage.\n",
      "Token Count:\n",
      " {'input_tokens': 803, 'output_tokens': 305, 'total_tokens': 1108}\n",
      "High level event:\n",
      " Net: invalidate\n",
      "\n",
      "High-Level Event: Cache line invalidation due to external request\n",
      "\n",
      "Logical Summary & Reasoning: The invalidate signal is triggered when an external request is made to invalidate a cache line, indicating that the data in the cache is no longer valid. This can occur due to various reasons such as a write operation from another core or a system-wide cache flush. From a micro-architectural perspective, this signal is trying to maintain cache coherence by ensuring that the data in the cache is consistent with the data in the main memory. At a high level, this event can be described as a cache line invalidation, which can lead to a cache miss if the same data is accessed again.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, software can use a combination of memory access instructions, such as loads and stores, to access a shared variable or data structure. Additionally, using synchronization primitives, such as locks or barriers, can also trigger cache line invalidation. For example, a C program can use the `pthread` library to create multiple threads that access a shared variable, which can lead to cache line invalidation.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal can be grouped into the following categories:\n",
      "- Load instructions: Instructions that access data from memory, such as `lw` or `ld`.\n",
      "- Store instructions: Instructions that write data to memory, such as `sw` or `st`.\n",
      "- Synchronization instructions: Instructions that synchronize access to shared data, such as `lock` or `barrier`.\n",
      "- Memory management instructions: Instructions that manage memory, such as `malloc` or `free`.\n",
      "Token Count:\n",
      " {'input_tokens': 803, 'output_tokens': 331, 'total_tokens': 1134}\n",
      "High level event:\n",
      " Net: invalidate_ack\n",
      "\n",
      "High-Level Event: Cache line invalidation acknowledgement received.\n",
      "\n",
      "Logical Summary & Reasoning: The invalidate_ack signal is related to the cache coherence protocol, specifically when a cache line is invalidated. The signal is asserted when the cache controller receives an acknowledgement from the cache that a cache line has been successfully invalidated. This is typically triggered by a snoop hit, which occurs when another core or agent in the system attempts to access a cache line that is currently cached by the local core. The cache controller then sends an invalidate request to the cache, and upon receiving the acknowledgement, the invalidate_ack signal is asserted. This signal is crucial in maintaining cache coherence and ensuring that all cores in the system have a consistent view of the memory.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the invalidate_ack signal, a software engineer can write a test program that performs the following steps: (1) allocate a shared variable in memory, (2) have one core cache the variable, (3) have another core attempt to access the variable, and (4) verify that the cache line is invalidated and the invalidate_ack signal is asserted. This can be achieved using a combination of load and store instructions, as well as cache control instructions (e.g., cache flush or invalidate).\n",
      "\n",
      "Instruction Categories: The instructions that influence the invalidate_ack signal can be grouped into the following categories: (1) load and store instructions that access shared variables, (2) cache control instructions (e.g., cache flush or invalidate), and (3) synchronization instructions (e.g., locks or barriers) that ensure coherence between multiple cores. Additionally, instructions that modify the cache configuration or control the cache coherence protocol (e.g., setting the cache snooping mode) can also impact the invalidate_ack signal.\n",
      "Token Count:\n",
      " {'input_tokens': 1572, 'output_tokens': 356, 'total_tokens': 1928}\n",
      "High level event:\n",
      " Net: invalidate_adr\n",
      "\n",
      "High-Level Event: Cache line invalidation due to a snoop request from another core or bus master.\n",
      "\n",
      "Logical Summary & Reasoning: The invalidate_adr signal is related to the cache coherence mechanism, specifically in response to a snoop request from another core or bus master. At a high level, this signal is trying to achieve cache coherence by invalidating a specific cache line, ensuring that the data is handled consistently across multiple cores or bus masters. The dependency on the snoop address input suggests that the signal is triggered when a snoop request is received for a specific address, which requires the cache line to be invalidated to maintain coherence.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, a software test can be designed to perform a series of memory accesses, including writes, to a specific address. Then, from another core or bus master, a snoop request can be initiated for the same address, triggering the invalidate_adr signal. The test can be written in C, using pointers to access memory locations, and utilizing multi-threading or multi-core programming to simulate the snoop request. For example, a test program can perform a write operation to a specific memory location, followed by a read operation from another core or thread, which can trigger the cache coherence mechanism and invalidate the cache line.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal can be grouped into categories such as: \n",
      "- Memory access instructions (e.g., load, store)\n",
      "- Cache maintenance instructions (e.g., cache flush, cache invalidate)\n",
      "- Synchronization instructions (e.g., lock, unlock)\n",
      "- Multi-threading or multi-core instructions (e.g., thread creation, synchronization primitives)\n",
      "Token Count:\n",
      " {'input_tokens': 805, 'output_tokens': 344, 'total_tokens': 1149}\n",
      "High level event:\n",
      " Net: next_refill_adr\n",
      "\n",
      "High-Level Event: Cache line refill address calculation.\n",
      "\n",
      "Logical Summary & Reasoning: The next_refill_adr signal is related to the calculation of the address for refilling a cache line. This calculation is dependent on the current state of the load/store unit and the type of operation being performed. The signal is influenced by various factors such as the cache block width, the current address, and the operation type. The calculation is performed to determine the next address to be refilled in the cache, taking into account the current address and the cache block width.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the next_refill_adr signal, a test program can be written to perform a series of load or store operations that access a large array or data structure, causing the cache to refill multiple times. The program can also include operations that change the cache block width or the current address, to test the signal's behavior under different conditions. Additionally, the program can include error handling and exception testing to verify the signal's behavior in the presence of errors or exceptions.\n",
      "\n",
      "Instruction Categories: The next_refill_adr signal is influenced by the following categories of instructions:\n",
      "* Load instructions (e.g. lw, lb, lh)\n",
      "* Store instructions (e.g. sw, sb, sh)\n",
      "* Cache control instructions (e.g. cache flush, cache invalidate)\n",
      "* Memory management instructions (e.g. page table updates, TLB reloads)\n",
      "* Error handling instructions (e.g. exception handling, error correction)\n",
      "Token Count:\n",
      " {'input_tokens': 4530, 'output_tokens': 310, 'total_tokens': 4840}\n",
      "High level event:\n",
      " Net: read\n",
      "\n",
      "High-Level Event: Pipeline stall due to data dependency on a load instruction.\n",
      "\n",
      "Logical Summary & Reasoning: The signal \"read\" is related to the loading of data from memory. When a load instruction is encountered, the processor checks if the required data is available. If the data is not available, the pipeline stalls until the data is loaded from memory. This stall is caused by a dependency on the load instruction, where the processor needs to wait for the data to be fetched before proceeding with the execution of subsequent instructions. The processor's load-store unit plays a crucial role in managing these dependencies and ensuring that data is handled correctly.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, software can use a sequence of instructions that include a load operation followed by an instruction that depends on the loaded data. For example, a C program can use a load operation to read data from an array, followed by an arithmetic operation that uses the loaded data. By carefully crafting the sequence of instructions, developers can create test cases that intentionally cause pipeline stalls due to data dependencies on load instructions.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal can be grouped into the following categories: \n",
      "1. Load instructions: Instructions that load data from memory, such as lw (load word) or ld (load doubleword).\n",
      "2. Store instructions: Instructions that store data to memory, such as sw (store word) or sd (store doubleword).\n",
      "3. Arithmetic instructions: Instructions that perform arithmetic operations, such as add, sub, mul, or div.\n",
      "4. Control-flow instructions: Instructions that change the flow of execution, such as branches or jumps, can also influence the pipeline stall event by changing the sequence of instructions that are executed.\n",
      "Token Count:\n",
      " {'input_tokens': 803, 'output_tokens': 351, 'total_tokens': 1154}\n",
      "High level event:\n",
      " Net: refill\n",
      "\n",
      "High-Level Event: Pipeline stall due to cache miss.\n",
      "\n",
      "Logical Summary & Reasoning: The refill signal is related to the cache subsystem, specifically when the processor encounters a cache miss. The signal is influenced by the address being accessed and the current state of the cache. When a cache miss occurs, the processor must stall the pipeline to wait for the data to be fetched from main memory, resulting in a pipeline stall. This event is a result of the processor's attempt to access data that is not currently resident in the cache, triggering a refill operation to fetch the required data.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, software can be written to access a large array or data structure that does not fit within the cache, causing frequent cache misses. Additionally, using pointers to access data in a non-sequential manner can also increase the likelihood of cache misses. The use of loops with large stride sizes or accessing data in a non-contiguous manner can also help trigger this event.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal include load and store instructions, particularly those that access large data structures or non-contiguous memory locations. Examples of such instructions include load-word, load-doubleword, store-word, and store-doubleword. Additionally, instructions that modify the cache behavior, such as cache flush or invalidate instructions, can also impact the refill signal.\n",
      "Token Count:\n",
      " {'input_tokens': 803, 'output_tokens': 277, 'total_tokens': 1080}\n",
      "High level event:\n",
      " Net: refill_adr_i\n",
      "\n",
      "High-Level Event: Pipeline stall due to data cache refill.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_adr_i signal is related to the processor's data cache subsystem, specifically when a cache miss occurs and the processor needs to refill the cache with data from main memory. This signal is likely triggered when the processor encounters a load instruction that misses in the cache, causing a stall in the pipeline until the data is fetched from memory. The pipeline stall is a result of the dependency on the data being loaded, which is not yet available in the cache.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, a software engineer can write a test program that performs a series of load instructions to a memory location that is not currently in the cache. This can be achieved by using a large array and accessing elements that are far apart, increasing the likelihood of cache misses. Additionally, using a loop that iterates over the array with a stride that is larger than the cache line size can also increase the chances of triggering a cache refill.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence this signal can be grouped into the following categories: \n",
      "- Load instructions (e.g., lw, ld) that access memory locations, \n",
      "- Store instructions (e.g., sw, sd) that may cause cache lines to be evicted, \n",
      "- Instructions that modify the cache behavior (e.g., cache flush, cache invalidate), \n",
      "- Instructions that access large data structures or arrays, increasing the likelihood of cache misses.\n",
      "Token Count:\n",
      " {'input_tokens': 806, 'output_tokens': 307, 'total_tokens': 1113}\n",
      "High level event:\n",
      " Net: refill_allowed_i\n",
      "\n",
      "High-Level Event: Cache refill is allowed when there is no ongoing store operation, no cache snoop hit, and no snoop validation is in progress.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_allowed_i signal is a result of evaluating several conditions related to the current state of the cache and the load/store unit. It checks if there is no ongoing store operation (ctrl_op_lsu_store_i is false), no cache snoop hit (dc_snoop_hit is false), and no snoop validation is in progress (snoop_valid is false). If all these conditions are met, the refill_allowed_i signal is asserted, indicating that a cache refill can be initiated. This signal is crucial in managing the cache coherence and ensuring that the cache is updated correctly.\n",
      "\n",
      "Test-Stimulus Guidance: To trigger the refill_allowed_i signal, a test program can be designed to perform a series of load operations that miss the cache, causing a cache refill. The program should also ensure that there are no ongoing store operations and no cache snoop hits. Additionally, the program can be designed to test the signal under different scenarios, such as when the cache is empty or when there are multiple outstanding load operations.\n",
      "\n",
      "Instruction Categories: The refill_allowed_i signal is influenced by the following categories of instructions:\n",
      "* Load instructions: These instructions can cause a cache refill when the requested data is not present in the cache.\n",
      "* Store instructions: Ongoing store operations can prevent the refill_allowed_i signal from being asserted.\n",
      "* Cache maintenance instructions: Instructions that modify the cache, such as cache flush or invalidate, can also affect the refill_allowed_i signal.\n",
      "* Synchronization instructions: Instructions that affect the cache coherence, such as lock or unlock, can also influence the refill_allowed_i signal.\n",
      "Token Count:\n",
      " {'input_tokens': 5309, 'output_tokens': 358, 'total_tokens': 5667}\n",
      "High level event:\n",
      " Net: refill_dat_i\n",
      "\n",
      "High-Level Event: Pipeline stall due to data cache refill.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_dat_i signal is related to the processor's data cache subsystem. When the signal is asserted, it indicates that the processor is experiencing a delay in fetching data from the cache, likely due to a cache miss. This delay can cause a pipeline stall, as the processor needs to wait for the data to be refetched from main memory. The signal's purpose is to manage the flow of data between the cache and the processor's execution pipeline, ensuring that data is available when needed. The processor's load-store unit is responsible for handling cache refills, and the refill_dat_i signal is likely used to coordinate this process.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke a pipeline stall due to data cache refill, software can use a combination of instructions that access data in a way that causes cache misses. For example, a program can use a loop to iterate over a large array, accessing elements in a non-sequential manner to increase the likelihood of cache misses. Additionally, using instructions that have a high latency, such as load instructions with a large offset, can also increase the likelihood of a cache refill.\n",
      "\n",
      "Instruction Categories: The types of instructions that influence the refill_dat_i signal can be grouped into the following categories: \n",
      "- Load instructions (e.g., lw, ld) that access data from memory\n",
      "- Store instructions (e.g., sw, sd) that write data to memory\n",
      "- Instructions that access large data structures or arrays, increasing the likelihood of cache misses\n",
      "- Instructions with high latency, such as those that access memory with a large offset.\n",
      "Token Count:\n",
      " {'input_tokens': 805, 'output_tokens': 338, 'total_tokens': 1143}\n",
      "High level event:\n",
      " Net: refill_done\n",
      "\n",
      "High-Level Event: Cache refill completion.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_done signal is related to the completion of a cache refill operation. This operation is triggered when the cache misses a requested data, and the processor needs to fetch the data from the main memory. The refill_done signal is set when the cache refill operation is completed, indicating that the requested data is now available in the cache. The signal is influenced by various factors, including the cache state, the memory access type (read or write), and the presence of any errors during the refill operation.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the refill_done signal, a software engineer can write a test program that performs a series of memory accesses, including reads and writes, to a specific memory location that is not currently cached. The program should also include a mix of cache hits and misses to exercise the cache refill mechanism. Additionally, the program can include error injection mechanisms to test the behavior of the refill_done signal in the presence of errors.\n",
      "\n",
      "Instruction Categories: The refill_done signal is influenced by the following categories of instructions: \n",
      "- Load instructions: These instructions can trigger a cache refill operation when the requested data is not in the cache.\n",
      "- Store instructions: These instructions can also trigger a cache refill operation when the requested data is not in the cache, and the store operation needs to be performed.\n",
      "- Memory access instructions: These instructions can influence the cache state and trigger cache refill operations.\n",
      "- Error-handling instructions: These instructions can be used to inject errors and test the behavior of the refill_done signal in the presence of errors.\n",
      "Token Count:\n",
      " {'input_tokens': 3722, 'output_tokens': 325, 'total_tokens': 4047}\n",
      "High level event:\n",
      " Net: refill_hit\n",
      "\n",
      "High-Level Event: Cache refill hit occurs when a requested data is found in the cache after a cache miss and refill operation.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_hit signal is triggered when the cache controller detects a hit in the cache after a refill operation. This happens when the cache controller is in the refill state, the refill address matches the current cache line, and there are no pending writes or errors. The cache controller checks for a hit by comparing the refill address with the current cache line address. If a hit is detected, the cache controller sets the refill_hit signal high, indicating that the requested data is available in the cache.\n",
      "\n",
      "Test-Stimulus Guidance: To trigger the refill_hit event, a test program can be designed to perform the following steps: \n",
      "1. Initialize the cache by loading data into it.\n",
      "2. Perform a cache miss by accessing data that is not in the cache.\n",
      "3. Trigger a cache refill operation by accessing the same data again.\n",
      "4. Verify that the refill_hit signal is asserted after the cache refill operation is complete.\n",
      "This can be achieved by using a combination of load and store instructions in a test program, with specific addresses and data values designed to trigger the cache miss and refill operations.\n",
      "\n",
      "Instruction Categories: The refill_hit signal is influenced by the following categories of instructions:\n",
      "1. Load instructions: These instructions can trigger a cache miss and subsequent refill operation.\n",
      "2. Store instructions: These instructions can also trigger a cache miss and refill operation, especially if the store address is not in the cache.\n",
      "3. Memory access instructions: Instructions that access memory locations can trigger cache misses and refills, depending on the memory address and the current state of the cache.\n",
      "4. Cache maintenance instructions: Instructions that explicitly manage the cache, such as cache flush or invalidate instructions, can also influence the refill_hit signal.\n",
      "Token Count:\n",
      " {'input_tokens': 6497, 'output_tokens': 375, 'total_tokens': 6872}\n",
      "High level event:\n",
      " Net: refill_req_o\n",
      "\n",
      "High-Level Event: The processor requests a cache refill due to a cache miss or an invalid cache line.\n",
      "\n",
      "Logical Summary & Reasoning: The `refill_req_o` signal is triggered when the processor encounters a cache miss or an invalid cache line, and the refill is allowed. This occurs when the processor is in the read state, the cache line is not valid, and there are no pending writes. The `refill_req_o` signal is also triggered when the processor is in the refill state. The signal is controlled by the cache controller, which checks the cache tags and determines whether a refill is necessary.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the `refill_req_o` signal, software can execute a series of instructions that access a cache line that is not present in the cache, such as a load instruction to a memory location that has not been accessed recently. Additionally, software can execute a sequence of instructions that invalidate the cache line, such as a store instruction to a memory location that is mapped to the same cache line.\n",
      "\n",
      "Instruction Categories: The instructions that influence the `refill_req_o` signal can be grouped into the following categories:\n",
      "* Load instructions that access a cache line that is not present in the cache\n",
      "* Store instructions that invalidate a cache line\n",
      "* Instructions that access a memory location that is mapped to the same cache line as a previous instruction\n",
      "* Instructions that execute a sequence of loads or stores that cause the cache controller to refill the cache line. Examples of such instructions include:\n",
      "\t+ Load instructions with a non-zero offset, such as `lw $t0, 0x10($t1)`\n",
      "\t+ Store instructions with a non-zero offset, such as `sw $t0, 0x10($t1)`\n",
      "\t+ Instructions that access a large array or data structure, such as a loop that loads or stores multiple elements of an array.\n",
      "Token Count:\n",
      " {'input_tokens': 6740, 'output_tokens': 389, 'total_tokens': 7129}\n",
      "High level event:\n",
      " Net: refill_valid\n",
      "\n",
      "High-Level Event: Cache refill operation is valid and can proceed.\n",
      "\n",
      "Logical Summary & Reasoning: The refill_valid signal is related to the cache refill operation, which is a critical component of the memory hierarchy. The signal is influenced by various factors, including the cache state, memory access requests, and error conditions. When the cache is in a valid state and a refill operation is requested, the refill_valid signal is asserted, indicating that the refill operation can proceed. This signal is crucial in ensuring that the cache is properly updated and that data consistency is maintained. The refill_valid signal is also dependent on the cache controller's state, which includes states such as idle, read, and refill.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke the refill_valid signal, software can execute a sequence of instructions that access memory locations, causing cache misses and subsequent refill operations. This can be achieved by writing a C program that performs a series of memory reads and writes, with the memory locations carefully chosen to cause cache misses. For example, the program can use a loop to access an array of integers, with the array size larger than the cache size. This will cause the cache to miss and refill, asserting the refill_valid signal. Additionally, the program can include instructions that modify the cache state, such as flushing the cache or invalidating cache lines, to further exercise the refill_valid signal.\n",
      "\n",
      "Instruction Categories: The refill_valid signal is influenced by various instruction categories, including:\n",
      "* Memory access instructions (e.g., load, store, fetch)\n",
      "* Cache management instructions (e.g., cache flush, cache invalidate)\n",
      "* Control flow instructions (e.g., branches, jumps) that can cause cache misses\n",
      "* Arithmetic and logical instructions that can modify memory locations and cause cache refills.\n",
      "Token Count:\n",
      " {'input_tokens': 1408, 'output_tokens': 358, 'total_tokens': 1766}\n",
      "High level event:\n",
      " Net: refill_we_i\n",
      "\n",
      "High-Level Event: Cache refill operation initiated due to a cache miss.\n",
      "\n",
      "Logical Summary & Reasoning: The signal `refill_we_i` is related to the cache refill operation, which occurs when the processor encounters a cache miss. The cache controller initiates a refill operation to fetch the required data from the main memory. This signal is likely influenced by the cache miss detection logic, which is triggered when the processor attempts to access a memory location that is not present in the cache. The cache controller then sends a request to the main memory to fetch the required data, and the `refill_we_i` signal is asserted to indicate that a refill operation is in progress.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, a test program can be written to perform a series of memory accesses that are likely to result in cache misses. This can be achieved by accessing a large array of data that is larger than the cache size, or by accessing data in a non-sequential manner to reduce the chances of cache hits. The test program can also include instructions that are known to generate cache misses, such as load instructions with a large offset or store instructions that write to a location that is not present in the cache.\n",
      "\n",
      "Instruction Categories: The instructions that are likely to influence the `refill_we_i` signal include:\n",
      "* Load instructions (e.g., lw, ld) that access memory locations that are not present in the cache\n",
      "* Store instructions (e.g., sw, sd) that write to memory locations that are not present in the cache\n",
      "* Instructions that generate cache misses due to non-sequential memory access patterns (e.g., indirect loads, stores with large offsets)\n",
      "* Instructions that access large arrays of data that are larger than the cache size\n",
      "* Instructions that perform memory accesses with a high degree of spatial or temporal locality, making it likely to result in cache misses.\n",
      "Token Count:\n",
      " {'input_tokens': 6342, 'output_tokens': 384, 'total_tokens': 6726}\n",
      "High level event:\n",
      " Net: snoop_adr_i\n",
      "\n",
      "High-Level Event: Pipeline stall due to cache coherence protocol.\n",
      "\n",
      "Logical Summary & Reasoning: The snoop_adr_i signal is related to the cache coherence protocol, which ensures that multiple cores or processors accessing the same memory location have a consistent view of the data. This signal is likely used to stall the pipeline when a coherence protocol request is received, indicating that another core is accessing the same memory location. The stall allows the core to wait for the coherence protocol to complete, ensuring that the data is handled correctly. The signal is influenced by the memory access patterns of the program and the interactions between multiple cores or processors.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke a pipeline stall due to cache coherence protocol, a test program can be written to perform the following operations: (1) initialize a shared variable in memory, (2) have one core or processor write to the shared variable, and (3) have another core or processor read from the same shared variable. The test program should be designed to maximize the likelihood of coherence protocol requests, such as by using multiple threads or processes to access the shared variable concurrently. Additionally, the test program can use synchronization primitives, such as locks or barriers, to control the timing of the memory accesses and increase the likelihood of pipeline stalls.\n",
      "\n",
      "Instruction Categories: The instructions that influence the snoop_adr_i signal can be grouped into the following categories: (1) load and store instructions, which access memory locations and can trigger coherence protocol requests, (2) synchronization instructions, such as locks and barriers, which can control the timing of memory accesses and increase the likelihood of pipeline stalls, and (3) instructions that modify memory locations, such as atomic operations, which can also trigger coherence protocol requests.\n",
      "Token Count:\n",
      " {'input_tokens': 807, 'output_tokens': 356, 'total_tokens': 1163}\n",
      "High level event:\n",
      " Net: snoop_check\n",
      "\n",
      "High-Level Event: Pipeline stall due to cache coherence issue.\n",
      "\n",
      "Logical Summary & Reasoning: The snoop_check signal is related to the cache coherence mechanism, which ensures that multiple cores or processors accessing shared memory have a consistent view of the data. This signal is likely triggered when a snoop request is received, indicating that another core is trying to access a memory location that is currently cached by the local core. The signal's purpose is to check if the snoop address matches a valid cache line, and if so, to initiate a cache coherence protocol to ensure data consistency. This process can lead to a pipeline stall as the core waits for the coherence protocol to complete.\n",
      "\n",
      "Test-Stimulus Guidance: To provoke this event, software can use a combination of memory access instructions, such as loads and stores, to shared memory locations. For example, a test program can have multiple threads or processes accessing the same memory location, with one thread writing to the location and another thread reading from it. The test program can also use synchronization primitives, such as locks or barriers, to increase the likelihood of cache coherence issues. Additionally, using instructions that access large blocks of memory or using non-temporal stores can also increase the likelihood of triggering this event.\n",
      "\n",
      "Instruction Categories: The types of instructions that can influence the snoop_check signal include:\n",
      "- Memory access instructions: loads, stores, load-linked, store-conditional\n",
      "- Synchronization instructions: locks, barriers, fences\n",
      "- Cache management instructions: cache flush, cache invalidate\n",
      "- Memory allocation and deallocation instructions: malloc, free\n",
      "- Instructions that access shared memory locations, such as those used in multi-threaded or multi-process programs.\n",
      "Token Count:\n",
      " {'input_tokens': 805, 'output_tokens': 343, 'total_tokens': 1148}\n",
      "Toal Token Count:\n",
      " 80291\n"
     ]
    }
   ],
   "source": [
    "results, store = semantic_search_with_history(\n",
    "                        #vector_store,\n",
    "                         filtered_net_list,\n",
    "                         module_hier,\n",
    "                         # 10,\n",
    "                         groq_chat,\n",
    "                         \"HIGH_LEVEL_EVENTS_LSU.json\",\n",
    "                         \"mor1kx_lsu_cappuccino\",\n",
    "                         \"./event_files/LLAMA_HIGH_LEVEL_EVENTS_LSU_60_90.txt\"\n",
    "                        )\n",
    "\n",
    "# with open(\"./event_files/LLAMA_HIGH_LEVEL_EVENTS_ALU_61_90_tmp.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     for v in results.values():\n",
    "#         high_level_event = v[0]\n",
    "#         print(\"High level event:\\n\", high_level_event)\n",
    "#         print(\"Token Count:\\n\", v[1])\n",
    "\n",
    "#         file.write(high_level_event + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bf296398-541d-4f9e-b48a-9612d18d0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'total_tokens': 48525\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def calculate_total_tokens(log_file_path):\n",
    "    total = 0\n",
    "    pattern = re.compile(r\"'total_tokens':\\s*(\\d+)\")\n",
    "    \n",
    "    with open(log_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                total += int(match.group(1))\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Example usage\n",
    "log_file_path = 'token_log.txt'  # Replace with your actual file path\n",
    "total_tokens_sum = calculate_total_tokens(log_file_path)\n",
    "print(f\"Total 'total_tokens': {total_tokens_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6e06b904-03f2-4109-aad0-71926f0e11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 24 entries to JSON and saved to ./event_files/HIGH_LEVEL_EVENTS_LSU_60_90.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "EVENT_FILE = \"HIGH_LEVEL_EVENTS_LSU_60_90\"\n",
    "# def convert_text_to_json(input_file, output_file):\n",
    "#     with open(input_file, 'r') as f:\n",
    "#         content = f.read()\n",
    "\n",
    "#     # Split the file into blocks (separated by two newlines)\n",
    "#     blocks = content.strip().split('\\n\\n')\n",
    "#     result = []\n",
    "\n",
    "#     for block in blocks:\n",
    "#         event = {}\n",
    "#         lines = block.strip().split('\\n')\n",
    "#         for line in lines:\n",
    "#             if line.startswith('Net:'):\n",
    "#                 event['Net'] = line.split(':', 1)[1].strip()\n",
    "#             elif line.startswith('High-Level Event:'):\n",
    "#                 event['High-Level Event'] = line.split(':', 1)[1].strip()\n",
    "#             elif line.startswith('High-Level Summary:'):\n",
    "#                 event['Logical Summary'] = line.split(':', 1)[1].strip()\n",
    "#             elif line.startswith('Reasoning:'):\n",
    "#                 event['Reasoning'] = line.split(':', 1)[1].strip()\n",
    "#         if event:\n",
    "#             result.append(event)\n",
    "\n",
    "#     # Write to JSON file\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         json.dump(result, f, indent=2)\n",
    "\n",
    "#     print(f\"✅ Converted {len(result)} entries to JSON and saved to {output_file}\")\n",
    "def convert_text_to_json(input_file, output_file):\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split content by 'Net:' while keeping the delimiter\n",
    "    blocks = content.split('Net:')[1:]  # Skip the first empty split\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().split('\\n')\n",
    "        event = {'Net': lines[0].strip()}\n",
    "        for idx, line in enumerate(lines[1:], start=1):\n",
    "            if line.startswith('High-Level Event:'):\n",
    "                event['High-Level Event'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('Logical Summary & Reasoning:'):\n",
    "                summary_lines = [line.split(':', 1)[1].strip()]\n",
    "                # Capture multi-line summary\n",
    "                for next_line in lines[idx+1:]:\n",
    "                    if next_line.startswith('Test-Stimulus Guidance:'):\n",
    "                        break\n",
    "                    summary_lines.append(next_line.strip())\n",
    "                event['Logical Summary & Reasoning'] = ' '.join(summary_lines).strip()\n",
    "                # break  # No need to process further lines\n",
    "            elif line.startswith('Test-Stimulus Guidance:'):\n",
    "                summary_lines = [line.split(':', 1)[1].strip()]\n",
    "                # Capture multi-line summary\n",
    "                for next_line in lines[idx+1:]:\n",
    "                    if next_line.startswith('Instruction Categories:'):\n",
    "                        break\n",
    "                    summary_lines.append(next_line.strip())\n",
    "                event['Test-Stimulus Guidance'] = ' '.join(summary_lines).strip()\n",
    "            elif line.startswith('Instruction Categories:'):\n",
    "                summary_lines = [line.split(':', 1)[1].strip()]\n",
    "                # Capture multi-line summary\n",
    "                for next_line in lines[idx+1:]:\n",
    "                    if next_line.startswith('Net:'):\n",
    "                        break\n",
    "                    summary_lines.append(next_line.strip())\n",
    "                event['Instruction Categories'] = ' '.join(summary_lines).strip()\n",
    "                break\n",
    "        result.append(event)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"✅ Converted {len(result)} entries to JSON and saved to {output_file}\")\n",
    "# Example usage\n",
    "convert_text_to_json(f'./event_files/{EVENT_FILE}.txt', f'./event_files/{EVENT_FILE}.json')\n",
    "\n",
    "# import csv\n",
    "\n",
    "# def json_to_libreoffice_csv(json_file, csv_file):\n",
    "#     with open(json_file, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # Open CSV writer\n",
    "#     with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         # Write headers\n",
    "#         writer.writerow(['High-Level Event', 'Net', 'Triggered?', 'Comments'])\n",
    "\n",
    "#         # Write data rows\n",
    "#         for item in data:\n",
    "#             writer.writerow([\n",
    "#                 item.get('High-Level Event', ''),\n",
    "#                 item.get('Net', ''),\n",
    "#                 '',  # Empty Triggered?\n",
    "#                 ''   # Empty Comments\n",
    "#             ])\n",
    "\n",
    "#     print(f\"✅ CSV saved to '{csv_file}' with {len(data)} entries.\")\n",
    "\n",
    "# # Example usage\n",
    "# json_to_libreoffice_csv(f'./event_files/{EVENT_FILE}.json', f'./event_files/{EVENT_FILE}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29d1e9-18ee-4ee7-8d59-d1c9d95d4bf4",
   "metadata": {},
   "source": [
    "### Creating knowledge base for a given set of nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5f38-008b-4aa7-b11c-4787ec616dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in store.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "#     print()\n",
    "#     print()\n",
    "for i,v in store.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f55f93-2530-4af0-bb20-0cfed40e95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# Function to perform semantic search and format the output\n",
    "def search_and_save(vector_store, net_name, hier, rareness_value, num_results, llm, output_file):\n",
    "    # Perform semantic search\n",
    "    result = semantic_search(vector_store, net_name, hier, num_results, llm)\n",
    "    if result is not None:\n",
    "        formatted_output = format_output(result)\n",
    "        \n",
    "        # Add the event rareness section with the value from the text file\n",
    "        formatted_output += f\"\\nEvent Rareness: The net '{net_name}' has a rareness value of {rareness_value}.\\n\"\n",
    "        \n",
    "        # Append to the output file\n",
    "        with open(output_file, \"a\") as f:\n",
    "            # f.write(f\"Net Name: {net_name}\\n\")\n",
    "            f.write(formatted_output)\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"Net \", net_name, \" has no logical expression in JSON!\")\n",
    "\n",
    "# Main processing loop\n",
    "def process_nets_file(input_file, vector_store, num_results, llm, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)  # Clear the file if it already exists\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split each line into net name and rareness value\n",
    "            columns = line.strip().split()\n",
    "            if len(columns) >= 2:  # Ensure both net name and rareness value are present\n",
    "                parts = columns[0].split('.')\n",
    "                net_name = parts[-1]\n",
    "                module_hier = '->'.join(parts[:-1])\n",
    "                rareness_value = columns[1]\n",
    "                search_and_save(vector_store, net_name, module_hier, rareness_value, num_results, llm, output_file)\n",
    "                # time.sleep(20)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"mor1kx_shortlisted_tmp.txt\"\n",
    "output_file = \"semantic_search_results_openAI.txt\"\n",
    "\n",
    "# Call the processing function\n",
    "process_nets_file(input_file, vector_store, 10, groq_chat, output_file)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41947782-4142-4008-a1f6-5621915e0e39",
   "metadata": {},
   "source": [
    "### Creating Test program process Using OpenAI API Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480c264-5410-4777-9ed6-f797df4bb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    # Triggered whenever the assistant generated a chunk of text during the streaming process\n",
    "    # It prints assistant > to the console and flushes the output to ensure it appears immediately\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    # Triggered when the assistant decides to call a \"tool.\" In the beta Assistant framework tools\n",
    "    # might be something like a search, a calculator, or a file reference\n",
    "    # It prints a line indicating the assistant is calling a certain tool type. For example, if the \n",
    "    # assistant calls a file search tool, you might see assistant > file search\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant >> {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    # This is triggered when the entire message from the assistant finished. In other words, once all chunks\n",
    "    # of text have been streamed and compiled into a final message\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text # Extracts message content\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "\n",
    "# Then, we use the stream SDK helper\n",
    "# with the EventHandler class to create the Run\n",
    "# and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    # instructions=\"Please address the user as Shuvo. The user has a premium account.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b8137-e0d0-4a4d-a842-eb64ed4c2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e74323-1326-456b-b5ab-8d95cb7c6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export REPLICATE_API_TOKEN="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880c8ac-0da5-40ad-9ea4-e681bb0f78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788997d5-3302-47ff-b875-dfcbd283cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = replicate.run(\n",
    "    \"meta/codellama-70b-instruct:a279116fe47a0f65701a8817188601e2fe8f4b9e04a518789655ea7b995851bf\",\n",
    "    input={\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 0.95,\n",
    "        \"prompt\": \"In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?\",\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.8,\n",
    "        \"system_prompt\": \"\",\n",
    "        \"repeat_penalty\": 1.1,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# The meta/codellama-70b-instruct model can stream output as it's running.\n",
    "# The predict method returns an iterator, and you can iterate over that output.\n",
    "for item in output:\n",
    "    # https://replicate.com/meta/codellama-70b-instruct/api#output-schema\n",
    "    print(item, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66428dcf-7a52-4ec3-883f-98fad769b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_net_for_event(file_path, target_event):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    result = None\n",
    "    for i in range(1, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if line.startswith(\"High-Level Event:\") and target_event in line:\n",
    "            prev_line = lines[i - 1].strip()\n",
    "            # print(f\"Previous line: {prev_line}\")\n",
    "            if prev_line.startswith(\"Net:\"):\n",
    "                result = prev_line.replace(\"Net: \", \"\").strip()\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39e0a70d-e2c9-4ce1-bede-3c5643478743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net name for event 'Conditional flag clearing during instruction execution': ctrl_flag_clear\n"
     ]
    }
   ],
   "source": [
    "project_root = \"/home/m588h354/projects/autophasew/openrisc/src\"\n",
    "event_root = \"/home/m588h354/projects/Rare_net_analysis-repo/event_identification\"\n",
    "stage = \"ctrl\"\n",
    "hle_file_name = \"HIGH_LEVEL_EVENTS_CTRL_11_30\"\n",
    "arch_event_dir = f\"{project_root}/architectural_events/{stage}_events\"\n",
    "event_details_file = f\"{event_root}/event_files/{hle_file_name}.txt\"\n",
    "json_file_retrieve = f\"{event_root}/event_files/{hle_file_name}.json\"\n",
    "event = 'Conditional flag clearing during instruction execution'\n",
    "net_name  = find_net_for_event(event_details_file, event)\n",
    "print(f\"Net name for event '{event}': {net_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e482200-a8f3-4037-8e1d-64284dbd6580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
